{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e22ae6f9-5c7b-4001-b219-4c619b7fc187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B disabled.\n"
     ]
    }
   ],
   "source": [
    "!wandb disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73fcf4cb-de71-42a6-abbc-e979bc6767cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa38e74b-7373-47b3-86b0-7c850db7c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "1. Read annotated multilingual ILI data using CustomDataset.\n",
    "2. Convert encoded features and labels to dataset objects for integration with transformers model training.\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "class CustomDataset(object):\n",
    "    def __init__(self, file_name):\n",
    "        \n",
    "        self._file_name = file_name        \n",
    "        self.data =  pd.read_csv(self._file_name)\n",
    "        self.tweets = self.data['tweet']\n",
    "        self.labels = self.data['final_annotation']\n",
    "\n",
    "    def __len__(self):    \n",
    "        if len(self.tweets) != len(self.labels):\n",
    "            raise sys.exit(f\"Number of tweets({len(self.tweets)}) and its labels({len(self.labels)}) do not match.\")\n",
    "        else:\n",
    "            return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets.iloc[idx] \n",
    "        label = self.labels.iloc[idx] \n",
    "        return tweet, label\n",
    "    \n",
    "    def getsplitidx(self, test_split=0.2, valid_split=None, group='lang', stratify_label='final_annotation', savepath=None, seed=42):\n",
    "        \n",
    "        # group day by language and then perform stratified split by categories and save indices as json\n",
    "        lang_split_idx = {}\n",
    "        for grp, grp_df in self.data.groupby(group): \n",
    "            print(f\"\\nProcessing data split for language: {grp}\")\n",
    "            train, test = train_test_split(grp_df, test_size=test_split, stratify=grp_df[stratify_label], random_state=seed)\n",
    "            if valid_split is not None:\n",
    "                # determine split size for validation on the remaining based on validation size required\n",
    "                n = self.data.shape[0]\n",
    "                valid_size = valid_split/(1-test_split)\n",
    "                # print(f\"{valid_split} of {n} is {valid_size} of \"train.shape[0]}\")\n",
    "                train, valid = train_test_split(train, test_size=valid_size, stratify=train[stratify_label], random_state=seed)\n",
    "                # print(f\"Distribution of train, valid and test set: {train.shape[0]}, {valid.shape[0]}, {test.shape[0]}\")\n",
    "            \n",
    "            print(f\"Distribution of classes in train set\\n{train[stratify_label].value_counts()}\")\n",
    "            print(f\"Distribution of classes in test set\\n{test[stratify_label].value_counts()}\")\n",
    "            lang_split_idx[grp] = {'train_idx':train.index.values.tolist(), \n",
    "                                   'test_idx':test.index.values.tolist()\n",
    "                                  }\n",
    "            if valid_split is not None:\n",
    "                print(f\"Distribution of classes in valid set\\n{valid[stratify_label].value_counts()}\\n\")\n",
    "                lang_split_idx[grp] = {'train_idx':train.index.values.tolist(), \n",
    "                                       'test_idx':test.index.values.tolist(), \n",
    "                                       'valid_idx':valid.index.values.tolist()\n",
    "                                      }\n",
    "                \n",
    "        if savepath is not None:\n",
    "            savepath = Path(savepath)\n",
    "            savepath.mkdir(parents=True, exist_ok=True)\n",
    "            with open(savepath.joinpath(\"split_idx.json\"), \"w\")  as f:\n",
    "                json.dump(lang_split_idx, f)\n",
    "        else:\n",
    "            return lang_split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25ac2b94-2394-4d7b-8f39-870908bd58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsplit(lang_split_idx, key='train_idx'):\n",
    "    idx_list = [v[key] for k,v in lang_split_idx.items()]\n",
    "    # print(len(idx_list))\n",
    "    idx_list = [i for eachlist in idx_list for i in eachlist]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc700a-eec1-49e1-8e79-13651c09f68e",
   "metadata": {},
   "source": [
    "## 1. Read data, split info and save split indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ef86224-6695-4b29-8d2a-7eb074b9f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "The script to split data into train, (validation) and test set\n",
    "Update required in final_configs.json \n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from pathlib import Path \n",
    "import argparse\n",
    "\n",
    "# original two category annotation\n",
    "# DATA_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/data/all/alldata.csv\")\n",
    "# PARAMS_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/final_configs.json\")\n",
    "# OUT_PATH = Path(\"/gaueko0/users/nmishra/multiling_fludetection/evals/evalnew\")\n",
    "\n",
    "# revised two category annotation\n",
    "DATA_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/data/all/alldata_revisedcateg.csv\")\n",
    "PARAMS_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/params.json\")\n",
    "OUT_PATH = Path(\"/gaueko0/users/nmishra/multiling_fludetection/final_evals/eval_revisedcateg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40dc57d3-9948-4976-b98b-5f26b7d1f95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in data: 4284\n",
      "Distribution of classes in all data\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    2587\n",
      "1. Likely ILI infection                        1697\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "tweets = CustomDataset(DATA_FILE)\n",
    "print(f\"Number of tweets in data: {tweets.__len__()}\")\n",
    "print(f\"Distribution of classes in all data\\n{tweets.labels.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c431245-5c3a-498f-a1d6-f809836d8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # original four category collapsed to two\n",
    "# # ------------------------------------------\n",
    "# # likey covid to likely ILI &\n",
    "# # ambigous to not related\n",
    "# # ------------------------------------------\n",
    "\n",
    "# # df = tweets.data\n",
    "# # print(df.shape)\n",
    "# # df['final_annotation'] = np.where(df['final_annotation']=='4. Ambiguous/Unsure', '3. Not Related to ILI or COVID-19 Infection',\n",
    "# #                           np.where(df['final_annotation']=='2. Likely COVID-19 Infection (after 2020 only)','1. Likely ILI infection',\n",
    "# #                                   df['final_annotation'])\n",
    "# #                           )\n",
    "# # df['final_annotation'].value_counts()\n",
    "# # df.to_csv(DATA_FILE.parent.joinpath(\"alldata_twocateg_nonrevised.csv\"))\n",
    "\n",
    "# DATA_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/data/all/alldata_twocateg.csv\")\n",
    "# OUT_PATH = Path(\"/gaueko0/users/nmishra/multiling_fludetection/evals/originalcateg/evalnew_twocateg\")\n",
    "# tweets = CustomDataset(DATA_FILE)\n",
    "# print(f\"Number of tweets in data: {tweets.__len__()}\")\n",
    "# print(f\"Distribution of classes in all data\\n{tweets.labels.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77d773ec-14ba-4921-9aea-01533052286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "split: [0.6, 0.2, 0.2]\n",
      "/gaueko0/users/nmishra/multiling_fludetection/final_evals/eval_revisedcateg/testset0.6_0.2_0.2\n",
      "\n",
      "Processing data split for language: de\n",
      "Distribution of classes in train set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    294\n",
      "1. Likely ILI infection                        285\n",
      "Name: count, dtype: int64\n",
      "Distribution of classes in test set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    99\n",
      "1. Likely ILI infection                        95\n",
      "Name: count, dtype: int64\n",
      "Distribution of classes in valid set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    99\n",
      "1. Likely ILI infection                        95\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Processing data split for language: en\n",
      "Distribution of classes in train set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    64\n",
      "1. Likely ILI infection                        53\n",
      "Name: count, dtype: int64\n",
      "Distribution of classes in test set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    22\n",
      "1. Likely ILI infection                        18\n",
      "Name: count, dtype: int64\n",
      "Distribution of classes in valid set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    21\n",
      "1. Likely ILI infection                        18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Processing data split for language: es\n",
      "Distribution of classes in train set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    360\n",
      "1. Likely ILI infection                        338\n",
      "Name: count, dtype: int64\n",
      "Distribution of classes in test set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    120\n",
      "1. Likely ILI infection                        113\n",
      "Name: count, dtype: int64\n",
      "Distribution of classes in valid set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    120\n",
      "1. Likely ILI infection                        113\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Processing data split for language: fr\n",
      "Distribution of classes in train set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    324\n",
      "1. Likely ILI infection                        252\n",
      "Name: count, dtype: int64\n",
      "Distribution of classes in test set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    108\n",
      "1. Likely ILI infection                         85\n",
      "Name: count, dtype: int64\n",
      "Distribution of classes in valid set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    108\n",
      "1. Likely ILI infection                         84\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Processing data split for language: it\n",
      "Distribution of classes in train set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    509\n",
      "1. Likely ILI infection                         88\n",
      "Name: count, dtype: int64\n",
      "Distribution of classes in test set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    170\n",
      "1. Likely ILI infection                         30\n",
      "Name: count, dtype: int64\n",
      "Distribution of classes in valid set\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    169\n",
      "1. Likely ILI infection                         30\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine number of splits required, split data and save\n",
    "with open (PARAMS_FILE, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "all_splits = []\n",
    "for split in params['SPLITS']:\n",
    "    print(f\"\\nsplit: {split}\") \n",
    "    dirname = f\"testset{'_'.join([str(i) for i in split])}\"    \n",
    "    split_path = OUT_PATH.joinpath(dirname)\n",
    "    print(split_path)\n",
    "    all_splits.append(tweets.getsplitidx(test_split=split[-1], valid_split=split[-2], savepath=split_path))\n",
    "    # all_splits.append(tweets.getsplitidx(test_split=split[-1], valid_split=split[-2], savepath=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4fe9ff-6b78-418a-8542-77989fa090dd",
   "metadata": {},
   "source": [
    "## 2. Read saved split indices and verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59ed8200-ef58-4b2e-82ad-79835a2fb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "The script to split data into train, (validation) and test set\n",
    "Update required in final_configs.json \n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from pathlib import Path \n",
    "import argparse\n",
    "\n",
    "# original four categ\n",
    "# -------------------\n",
    "# DATA_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/data/all/alldata.csv\")\n",
    "# PARAMS_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/final_configs.json\")\n",
    "# OUT_PATH = Path(\"/gaueko0/users/nmishra/multiling_fludetection/evalnew\")\n",
    "\n",
    "# DATA_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/data/all/alldata_twocateg.csv\")\n",
    "# OUT_PATH = Path(\"/gaueko0/users/nmishra/multiling_fludetection/evals/originalcateg/evalnew_twocateg\")\n",
    "\n",
    "# revised two categ\n",
    "# -------------------\n",
    "DATA_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/data/all/alldata_revisedcateg.csv\")\n",
    "PARAMS_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/params.json\")\n",
    "OUT_PATH = Path(\"/gaueko0/users/nmishra/multiling_fludetection/evals/eval_revisedcateg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b9edad6-f74b-43a6-b2e5-eedb4affde49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4284, 6)\n",
      "Number of tweets in data: 4284\n",
      "Distribution of classes in all data\n",
      "final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    2587\n",
      "1. Likely ILI infection                        1697\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "tweets = CustomDataset(DATA_FILE)\n",
    "print(tweets.data.shape)\n",
    "print(f\"Number of tweets in data: {tweets.__len__()}\")\n",
    "print(f\"Distribution of classes in all data\\n{tweets.labels.value_counts()}\")\n",
    "\n",
    "# determine number of splits required, split data and save\n",
    "with open (PARAMS_FILE, \"r\") as f:\n",
    "    params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1691d70c-dc3d-4ae1-9c41-2b52b928783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6, 0.2, 0.2]\n",
      "Reading data split index from: /gaueko0/users/nmishra/multiling_fludetection/evals/eval_revisedcateg/testset0.6_0.2_0.2\n",
      "Training data used for all languages\n",
      "Distribution of data in train, validation and test splits: 2567, 857, 860\n"
     ]
    }
   ],
   "source": [
    "for split in params['SPLITS'][:1]:\n",
    "    print(split)\n",
    "    # data split index info\n",
    "    dirname = f\"testset{'_'.join([str(i) for i in split])}\"\n",
    "    split_path = OUT_PATH.joinpath(dirname)\n",
    "    print(f\"Reading data split index from: {split_path}\")\n",
    "    with open(split_path.joinpath('split_idx.json'), 'r') as f:\n",
    "        split_idx = json.load(f) \n",
    "        \n",
    "    # determine languages for which to get split index\n",
    "    if params['LANG']=='all':\n",
    "        languages = [i for i in split_idx]\n",
    "    else:\n",
    "        languages = [i for i in split_idx if i in params['LANG'].split(',')]\n",
    "    \n",
    "    # train on all languages and then on each language\n",
    "    lang_split_idx = {i:split_idx[i] for i in languages}\n",
    "    print(f\"Training data used for {params['LANG']} languages\")\n",
    "\n",
    "    # get train, valid and test split for selected languages\n",
    "    train_idx = getsplit(lang_split_idx, key='train_idx')\n",
    "    valid_idx = getsplit(lang_split_idx, key='valid_idx')\n",
    "    test_idx = getsplit(lang_split_idx, key='test_idx')\n",
    "    print(f\"Distribution of data in train, validation and test splits: {len(train_idx)}, {len(valid_idx)}, {len(test_idx)}\")\n",
    "\n",
    "    # inspect data\n",
    "    train_df = tweets.data.iloc[train_idx]\n",
    "    valid_df = tweets.data.iloc[valid_idx]\n",
    "    test_df = tweets.data.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58b2124d-d8be-4c19-833b-ae9c4b4e1d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(train_df['id'].duplicated().sum(), valid_df['id'].duplicated().sum(), test_df['id'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0eff43f9-d065-4f7c-ab95-a31f9845ff6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "es    698\n",
       "it    597\n",
       "de    579\n",
       "fr    576\n",
       "en    117\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4f42be1-60f5-4754-8477-80bcaf0c5a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "es    233\n",
       "it    200\n",
       "de    194\n",
       "fr    193\n",
       "en     40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b046b69-6e09-4c62-b459-edc84e150869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "es    233\n",
       "it    199\n",
       "de    194\n",
       "fr    192\n",
       "en     39\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35ef81-a1c6-4e83-94c0-3c46fe9ad5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796f719-0af5-4b3c-a186-630c93742c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel ILI Test",
   "language": "python",
   "name": "ipykernel-ili-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
