{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355bf93c-f551-48f3-98ff-322d83a113be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  4 23:38:35 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.98                 Driver Version: 535.98       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   20C    P0              26W / 165W |    252MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A30                     Off | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              25W / 165W |     21MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     14248      G   /usr/libexec/Xorg                             4MiB |\n",
      "|    0   N/A  N/A   3649968      C   ...anaconda3/envs/rag-llama/bin/python      226MiB |\n",
      "|    1   N/A  N/A     14248      G   /usr/libexec/Xorg                             4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e00047-3e87-4328-8cb0-f6a503b835aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bab054-3d6f-4e13-8a11-1bd88ac9b0ca",
   "metadata": {},
   "source": [
    "# Go to Table of Contents\n",
    "\n",
    "1. [Go to TOC](#gototoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a25cdc95-89d6-48a2-b372-9e6c685dee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsplit(lang_split_idx, key='train_idx'):\n",
    "    idx_list = [v[key] for k,v in lang_split_idx.items()]\n",
    "    # print(len(idx_list))\n",
    "    idx_list = [i for eachlist in idx_list for i in eachlist]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd25941-7ae2-458f-9410-a85aedc0266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4b9a8c-4983-48f8-a491-5a133bb630da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/gaueko0/users/nmishra/multiling_fludetection/final_evals/eval_revisedcateg')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revised = True\n",
    "newsplit = True\n",
    "filename = '_revisedcateg' if revised else ''\n",
    "folder = 'final_' if newsplit else ''\n",
    "\n",
    "ROOT = Path('/gaueko0/users/nmishra/multiling_fludetection')\n",
    "DATA_PATH = ROOT.joinpath('data')\n",
    "PARAMS_FILE = ROOT.joinpath('params_revisedcateg.tsv')\n",
    "\n",
    "OUT_PATH = ROOT.joinpath(f'{folder}evals/eval_revisedcateg')\n",
    "OUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8611f1a2-5677-4315-8156-df924776f94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4284, 6)\n",
      "(21420, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>target_lang</th>\n",
       "      <th>lang</th>\n",
       "      <th>year</th>\n",
       "      <th>final_annotation</th>\n",
       "      <th>overlap</th>\n",
       "      <th>eval_lang</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21850866789130240</td>\n",
       "      <td>Hola Twitter. En 2010 casi me muero de gripe, ...</td>\n",
       "      <td>es</td>\n",
       "      <td>fr</td>\n",
       "      <td>2011</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-es</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21850866789130240</td>\n",
       "      <td>Hallo Twitter. 2010 bin ich fast an einer Grip...</td>\n",
       "      <td>de</td>\n",
       "      <td>fr</td>\n",
       "      <td>2011</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-de</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21850866789130240</td>\n",
       "      <td>Hello Twitter. In 2010, I almost died of the f...</td>\n",
       "      <td>en</td>\n",
       "      <td>fr</td>\n",
       "      <td>2011</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-en</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21850866789130240</td>\n",
       "      <td>Ciao Twitter. Nel 2010 ho rischiato di morire ...</td>\n",
       "      <td>it</td>\n",
       "      <td>fr</td>\n",
       "      <td>2011</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-it</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21850866789130240</td>\n",
       "      <td>Bonjour Twitter. En 2010, j'ai frôlé la mort p...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>2011</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-fr</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                              tweet  \\\n",
       "0  21850866789130240  Hola Twitter. En 2010 casi me muero de gripe, ...   \n",
       "1  21850866789130240  Hallo Twitter. 2010 bin ich fast an einer Grip...   \n",
       "2  21850866789130240  Hello Twitter. In 2010, I almost died of the f...   \n",
       "3  21850866789130240  Ciao Twitter. Nel 2010 ho rischiato di morire ...   \n",
       "4  21850866789130240  Bonjour Twitter. En 2010, j'ai frôlé la mort p...   \n",
       "\n",
       "  target_lang lang  year         final_annotation  overlap eval_lang  original  \n",
       "0          es   fr  2011  1. Likely ILI infection        0     fr-es     False  \n",
       "1          de   fr  2011  1. Likely ILI infection        0     fr-de     False  \n",
       "2          en   fr  2011  1. Likely ILI infection        0     fr-en     False  \n",
       "3          it   fr  2011  1. Likely ILI infection        0     fr-it     False  \n",
       "4          fr   fr  2011  1. Likely ILI infection        0     fr-fr      True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "all_data = pd.read_csv(DATA_PATH.joinpath(f'all/alldata{filename}.csv'))\n",
    "print(all_data.shape)\n",
    "\n",
    "full_data = pd.read_csv(DATA_PATH.joinpath(f'all/fulldata{filename}.csv'))\n",
    "full_data = full_data.convert_dtypes()\n",
    "\n",
    "print(full_data.shape)\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8420ebc-676e-47fc-97a3-3f25e42fc149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.6,0.2,0.2'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load splits for all data\n",
    "params = pd.read_csv(PARAMS_FILE, sep='\\t')\n",
    "SPLITS = params['split'].unique()\n",
    "SPLITS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb03a6f-5a80-4bd3-b3e4-3f3a2746fcfe",
   "metadata": {},
   "source": [
    "## A. Get the ids associated with each split per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e41f9f-bd1b-473f-a0d8-2b14be21c750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6,0.2,0.2\n",
      "Reading data split index from: /gaueko0/users/nmishra/multiling_fludetection/final_evals/eval_revisedcateg/testset0.6_0.2_0.2\n",
      "de\n",
      "  train_idx\n",
      "  test_idx\n",
      "  valid_idx\n",
      "en\n",
      "  train_idx\n",
      "  test_idx\n",
      "  valid_idx\n",
      "es\n",
      "  train_idx\n",
      "  test_idx\n",
      "  valid_idx\n",
      "fr\n",
      "  train_idx\n",
      "  test_idx\n",
      "  valid_idx\n",
      "it\n",
      "  train_idx\n",
      "  test_idx\n",
      "  valid_idx\n"
     ]
    }
   ],
   "source": [
    "# for split in SPLITS:\n",
    "split = SPLITS[0]\n",
    "print(split)\n",
    "\n",
    "# read data split index\n",
    "dirname = f\"testset{split.replace(',','_')}\"\n",
    "split_path = OUT_PATH.joinpath(dirname)\n",
    "print(f\"Reading data split index from: {split_path}\")\n",
    "with open(split_path.joinpath('split_idx.json'), 'r') as f:\n",
    "    split_idx = json.load(f) \n",
    "\n",
    "# get associated ids\n",
    "split_id = {}\n",
    "for lang, splits_dict in split_idx.items():\n",
    "    lang_split_id = {}\n",
    "    print(lang)\n",
    "    for split_type, idx in splits_dict.items():\n",
    "        print(f'  {split_type}')\n",
    "        sub_df = all_data.iloc[idx]['id'].values\n",
    "        lang_split_id[split_type] = all_data.iloc[idx]['id'].values\n",
    "    split_id[lang] = lang_split_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cbfffd7-6c58-409d-a51b-7a09bdda0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if any index is duplicated\n",
    "# test_idx_df = pd.DataFrame.from_dict(split_idx)\n",
    "# for col in test_idx_df.columns:\n",
    "#     print(col, test_idx_df[[col]].explode(col).duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c81a5b71-0360-480e-885f-9ee8f3884ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check each index is uniquely placed in one split type of one language only\n",
    "# for i in range(21):\n",
    "#     n = np.random.randint(0, all_data.shape[0])\n",
    "#     print(n)\n",
    "#     test_idx_df = pd.DataFrame.from_dict(split_idx)\n",
    "#     test_idx_df = test_idx_df[test_idx_df.map(lambda x: n in x)]\n",
    "#     if (test_idx_df.notna().sum().sum()==1) or test_idx_df.notna().sum().sum()==0:\n",
    "#         print('matched only once or not at all')\n",
    "#     else:\n",
    "#         print('matched more than once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d78f9d7-e49a-408e-bf9b-9ec4ade72013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if any id is duplicated\n",
    "# test_id_df = pd.DataFrame.from_dict(split_id)\n",
    "# for col in test_id_df.columns:\n",
    "#     print(col, test_id_df[[col]].explode(col).duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03cb4481-c165-493b-bf72-b168249b3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check each id is uniquely placed in one split type of one language only\n",
    "# for i in range(21):\n",
    "#     n = np.random.randint(0, all_data.shape[0])\n",
    "#     print(n)\n",
    "#     test_id_df = pd.DataFrame.from_dict(split_id)\n",
    "#     test_id_df = test_id_df[test_id_df.map(lambda x: n in x)]\n",
    "#     if (test_id_df.notna().sum().sum()==1) or test_id_df.notna().sum().sum()==0:\n",
    "#         print('matched only once or not at all')\n",
    "#     else:\n",
    "#         print('matched more than once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717825af-8bb4-4547-8b57-521b9bdec72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of data in train, validation and test splits: 2567, 857, 860\n"
     ]
    }
   ],
   "source": [
    "train_ids = getsplit(split_id, key='train_idx')\n",
    "valid_ids = getsplit(split_id, key='valid_idx')\n",
    "test_ids  = getsplit(split_id, key='test_idx')\n",
    "print(f\"Distribution of data in train, validation and test splits: {len(train_ids)}, {len(valid_ids)}, {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55b45280-65c9-4425-ad05-4d6a2cfadfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang  final_annotation                           \n",
       "de    3. Not Related to ILI or COVID-19 Infection     99\n",
       "      1. Likely ILI infection                         95\n",
       "en    3. Not Related to ILI or COVID-19 Infection     22\n",
       "      1. Likely ILI infection                         18\n",
       "es    3. Not Related to ILI or COVID-19 Infection    120\n",
       "      1. Likely ILI infection                        113\n",
       "fr    3. Not Related to ILI or COVID-19 Infection    108\n",
       "      1. Likely ILI infection                         85\n",
       "it    3. Not Related to ILI or COVID-19 Infection    170\n",
       "      1. Likely ILI infection                         30\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = all_data[all_data['id'].isin(test_ids)]\n",
    "test_data.groupby('lang')['final_annotation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4965ecd-9569-4d00-b0dd-0bb3d34f00ca",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "<a name=\"gototoc\"></a>\n",
    "1. [Original](#original)\n",
    "2. [Learning Curve](#learning_curve)\n",
    "3. [One to Many](#onetomany)\n",
    "4. [Many to One](#manytoone)\n",
    "5. [Train Original Test Many to One](#trainoriginaltestmanytoone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e1b0f-a548-469a-8fad-624115f155b5",
   "metadata": {},
   "source": [
    "<a name=\"original\"></a>\n",
    "\n",
    "## 1 Original\n",
    "### i. Get split index for each split of each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a72f262b-cb96-4ab6-b48f-73223868be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_type = 'original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba798097-7430-4b12-b024-85668fd1a472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de train_idx (579, 9)\n",
      "de test_idx (4300, 9)\n",
      "de valid_idx (194, 9)\n",
      "-------------\n",
      "en train_idx (117, 9)\n",
      "en test_idx (4300, 9)\n",
      "en valid_idx (39, 9)\n",
      "-------------\n",
      "es train_idx (698, 9)\n",
      "es test_idx (4300, 9)\n",
      "es valid_idx (233, 9)\n",
      "-------------\n",
      "fr train_idx (576, 9)\n",
      "fr test_idx (4300, 9)\n",
      "fr valid_idx (192, 9)\n",
      "-------------\n",
      "it train_idx (597, 9)\n",
      "it test_idx (4300, 9)\n",
      "it valid_idx (199, 9)\n",
      "-------------\n",
      "dict_keys(['de', 'en', 'es', 'fr', 'it'])\n"
     ]
    }
   ],
   "source": [
    "translate_test = True\n",
    "original_split_idx = {}\n",
    "\n",
    "for lang, splits_dict in split_id.items():\n",
    "    original_lang_split_idx = {}\n",
    "    \n",
    "    for split_type, ids in splits_dict.items():                           # for each split in each language\n",
    "        \n",
    "        if 'test' in split_type:                                          # if test\n",
    "            sub_df = full_data[full_data['id'].isin(test_ids)]            # take all test ids\n",
    "            # sub_df = sub_df[sub_df['lang']==sub_df['target_lang']]        # take only original\n",
    "        \n",
    "        else:                                                             # otherwise\n",
    "            sub_df = full_data[full_data['id'].isin(ids)]                 # take split type ids for the language in loop only\n",
    "            sub_df = sub_df[sub_df['lang']==sub_df['target_lang']]        # take only original\n",
    "        print(lang, split_type, sub_df.shape)\n",
    "        original_lang_split_idx[split_type] = sub_df.index.tolist()       # save the index of the selected ids\n",
    "        \n",
    "    original_split_idx[lang] = original_lang_split_idx\n",
    "    print('-'*13)\n",
    "\n",
    "print(original_split_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10628fda-1bb8-4ef3-999c-c199b33e8eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_idx', 'test_idx', 'valid_idx'])\n",
      "test_idx\n",
      "5943\n",
      "matched more than once\n",
      "test_idx\n",
      "16173\n",
      "matched more than once\n",
      "test_idx\n",
      "7306\n",
      "matched more than once\n",
      "test_idx\n",
      "18516\n",
      "matched more than once\n"
     ]
    }
   ],
   "source": [
    "# check each index is uniquely placed in one split type of one language only\n",
    "# test_set will have repeat here \n",
    "all_split_types = original_split_idx.get('de').keys()\n",
    "print(all_split_types)\n",
    "\n",
    "for type_to_test in all_split_types:\n",
    "    for i in range(21):\n",
    "        n = np.random.randint(0, full_data.shape[0])\n",
    "        test_df = pd.DataFrame.from_dict(original_split_idx)\n",
    "        test_df = test_df[test_df.index==type_to_test]\n",
    "        test_df = test_df[test_df.map(lambda x: n in x)]\n",
    "        if (test_df.notna().sum().sum()==1) or test_df.notna().sum().sum()==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(type_to_test)\n",
    "            print(n)\n",
    "            print('matched more than once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9ae0c85-5fac-48e4-892f-5f4ff30b1208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>target_lang</th>\n",
       "      <th>lang</th>\n",
       "      <th>year</th>\n",
       "      <th>final_annotation</th>\n",
       "      <th>overlap</th>\n",
       "      <th>eval_lang</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>1043436583578423296</td>\n",
       "      <td>L 'AMORE NON È POSSESSO.\n",
       "\n",
       "Io sono ogni giorno ...</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>2018</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>1</td>\n",
       "      <td>it-it</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>1045676821977665541</td>\n",
       "      <td>#LaPalabraDelDía di oggi è \"almena\", questi sp...</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>2018</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>1</td>\n",
       "      <td>it-it</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>1047834383867158529</td>\n",
       "      <td>Il mio prof mi ha accattato l'influenza, GRAZI...</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>2018</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>it-it</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>1059752080095744005</td>\n",
       "      <td>@user Tranquillo, appena realizzi che non può ...</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>2018</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>1</td>\n",
       "      <td>it-it</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>1060245545824989184</td>\n",
       "      <td>buona cena.\n",
       "stasera brodino vegetale e patale ...</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>2018</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>1</td>\n",
       "      <td>it-it</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21053</th>\n",
       "      <td>1630860808329494528</td>\n",
       "      <td>@user @user @user C'entra molto più della tua ...</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>2023</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>it-it</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21112</th>\n",
       "      <td>1631635285422268418</td>\n",
       "      <td>Questo nuovo ordine mondiale multipolare è car...</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>2023</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>it-it</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21232</th>\n",
       "      <td>1633444670914146304</td>\n",
       "      <td>@user OK. San Marino e Vaticano sono nella zon...</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>2023</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>it-it</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21370</th>\n",
       "      <td>1635581813409161216</td>\n",
       "      <td>@user @user @user uno dei problemi è capire (e...</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>2023</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>it-it</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21409</th>\n",
       "      <td>1635992287149686784</td>\n",
       "      <td>Mi conforta sapere che il grande Prof #Parsi m...</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>2023</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>it-it</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                              tweet  \\\n",
       "3106   1043436583578423296  L 'AMORE NON È POSSESSO.\n",
       "\n",
       "Io sono ogni giorno ...   \n",
       "3309   1045676821977665541  #LaPalabraDelDía di oggi è \"almena\", questi sp...   \n",
       "3440   1047834383867158529  Il mio prof mi ha accattato l'influenza, GRAZI...   \n",
       "3971   1059752080095744005  @user Tranquillo, appena realizzi che non può ...   \n",
       "4023   1060245545824989184  buona cena.\n",
       "stasera brodino vegetale e patale ...   \n",
       "...                    ...                                                ...   \n",
       "21053  1630860808329494528  @user @user @user C'entra molto più della tua ...   \n",
       "21112  1631635285422268418  Questo nuovo ordine mondiale multipolare è car...   \n",
       "21232  1633444670914146304  @user OK. San Marino e Vaticano sono nella zon...   \n",
       "21370  1635581813409161216  @user @user @user uno dei problemi è capire (e...   \n",
       "21409  1635992287149686784  Mi conforta sapere che il grande Prof #Parsi m...   \n",
       "\n",
       "      target_lang lang  year                             final_annotation  \\\n",
       "3106           it   it  2018  3. Not Related to ILI or COVID-19 Infection   \n",
       "3309           it   it  2018  3. Not Related to ILI or COVID-19 Infection   \n",
       "3440           it   it  2018                      1. Likely ILI infection   \n",
       "3971           it   it  2018  3. Not Related to ILI or COVID-19 Infection   \n",
       "4023           it   it  2018                      1. Likely ILI infection   \n",
       "...           ...  ...   ...                                          ...   \n",
       "21053          it   it  2023  3. Not Related to ILI or COVID-19 Infection   \n",
       "21112          it   it  2023  3. Not Related to ILI or COVID-19 Infection   \n",
       "21232          it   it  2023  3. Not Related to ILI or COVID-19 Infection   \n",
       "21370          it   it  2023  3. Not Related to ILI or COVID-19 Infection   \n",
       "21409          it   it  2023  3. Not Related to ILI or COVID-19 Infection   \n",
       "\n",
       "       overlap eval_lang  original  \n",
       "3106         1     it-it      True  \n",
       "3309         1     it-it      True  \n",
       "3440         0     it-it      True  \n",
       "3971         1     it-it      True  \n",
       "4023         1     it-it      True  \n",
       "...        ...       ...       ...  \n",
       "21053        0     it-it      True  \n",
       "21112        0     it-it      True  \n",
       "21232        0     it-it      True  \n",
       "21370        0     it-it      True  \n",
       "21409        0     it-it      True  \n",
       "\n",
       "[199 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_type = 'test_idx'\n",
    "split_type = 'train_idx'\n",
    "split_type = 'valid_idx'\n",
    "language = 'it'\n",
    "full_data[full_data.index.isin(original_split_idx[language][split_type])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b090a06-4230-42fb-9e6e-33bc2d62b4ae",
   "metadata": {},
   "source": [
    "### ii. Get split index for each split for all languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aad525cc-d0b9-4c86-8bac-3a2b6cc7e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2567, 4300, 857]\n",
      "dict_keys(['de', 'en', 'es', 'fr', 'it', 'all'])\n"
     ]
    }
   ],
   "source": [
    "original_lang_split_idx = {}\n",
    "\n",
    "original_df = full_data[full_data['lang']==full_data['target_lang']]\n",
    "translated_df = full_data[full_data['lang']!=full_data['target_lang']]\n",
    "\n",
    "original_lang_split_idx['train_idx'] = original_df[original_df['id'].isin(train_ids)].index.tolist()                # take all test ids\n",
    "original_lang_split_idx['test_idx']  = full_data[full_data['id'].isin(test_ids)].index.tolist()\n",
    "original_lang_split_idx['valid_idx'] = original_df[original_df['id'].isin(valid_ids)].index.tolist()\n",
    "print([len(i) for i in original_lang_split_idx.values()])\n",
    "\n",
    "original_split_idx['all'] = original_lang_split_idx\n",
    "print(original_split_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c64beab8-c035-49fc-9b48-29cd78032a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gaueko0/users/nmishra/multiling_fludetection/final_evals/eval_revisedcateg/testset0.6_0.2_0.2/split_idx_original.json\n"
     ]
    }
   ],
   "source": [
    "# # save the split index\n",
    "print(split_path.joinpath(f'split_idx_{experiment_type}.json'))\n",
    "with open(split_path.joinpath(f'split_idx_{experiment_type}.json'), 'w') as f:\n",
    "    json.dump(original_split_idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39128177-56e6-4250-a8c3-b5f9fc1094fd",
   "metadata": {},
   "source": [
    "<a name=\"learning_curve\"></a>\n",
    "\n",
    "## 2 Learning Curve\n",
    "### Get split index for one language for each fraction/partition of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2a2085-42a1-4400-91ef-09cbb5a240bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'es'\n",
    "experiment_type = f'original_{language}_learningcurve'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1200e98a-6944-4765-beff-91b6c036053e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'es_one': {'train_idx': 174, 'test_idx': 58, 'valid_idx': 58}, 'es_two': {'train_idx': 349, 'test_idx': 116, 'valid_idx': 116}, 'es_three': {'train_idx': 488, 'test_idx': 163, 'valid_idx': 163}}\n"
     ]
    }
   ],
   "source": [
    "fractions = [0.25, 0.50, .70]\n",
    "fractions_name = ['one', 'two', 'three']\n",
    "n_splits = len(fractions)\n",
    "lang_split_idx = split_idx[language]\n",
    "fraction_idx_len = {}\n",
    "for i in range(n_splits):\n",
    "    fraction_idx_len[f'{language}_{fractions_name[i]}'] = {split_type: int( len( lang_split_idx[split_type] )*fractions[i] ) \n",
    "                                                           for split_type in lang_split_idx}\n",
    "print(fraction_idx_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb0288bb-1bd6-4689-b32b-0e891524ffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es_one\n",
      "train_idx 174\n",
      "test_idx 4300\n",
      "valid_idx 58\n",
      "es_two\n",
      "train_idx 349\n",
      "test_idx 4300\n",
      "valid_idx 116\n",
      "es_three\n",
      "train_idx 488\n",
      "test_idx 4300\n",
      "valid_idx 163\n"
     ]
    }
   ],
   "source": [
    "fractional_split_idx = {}\n",
    "for each_split in fraction_idx_len:\n",
    "    print(each_split)\n",
    "    idx_len = fraction_idx_len[each_split]\n",
    "    each_split_idx = {}\n",
    "    for split_type, split_type_ids in split_id[language].items():\n",
    "        if 'test' in split_type:\n",
    "            each_split_idx[split_type] = full_data[full_data['id'].isin(test_ids)].index.tolist()  # take all test ids\n",
    "        else:\n",
    "            sub_df = full_data[full_data['id'].isin(split_type_ids)]                         # take the associated ids in full data\n",
    "            each_split_ids = sub_df[sub_df['target_lang']==sub_df['lang']].index.tolist()    # take index of original items only\n",
    "            each_split_idx[split_type] = each_split_ids[ :idx_len[split_type]]               # get fraction of idx\n",
    "        fractional_split_idx[each_split] = each_split_idx\n",
    "        print(split_type, len(each_split_idx[split_type]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9416329b-3c3d-4b59-911e-aac0fce6a26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>target_lang</th>\n",
       "      <th>lang</th>\n",
       "      <th>year</th>\n",
       "      <th>final_annotation</th>\n",
       "      <th>overlap</th>\n",
       "      <th>eval_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>1041876687988948993</td>\n",
       "      <td>Yo sé que las mujeres tienen muchísimas aptitu...</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>2018</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>es-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3160</th>\n",
       "      <td>1044350928227364869</td>\n",
       "      <td>Hace como 1 mes que estoy con esta gripe del o...</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>2018</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>1</td>\n",
       "      <td>es-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>1044540627491475456</td>\n",
       "      <td>Que mañana vaya alguien de mi parte a currar, ...</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>2018</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>es-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266</th>\n",
       "      <td>1045317807263617024</td>\n",
       "      <td>Tengo una gripe...</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>2018</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>1</td>\n",
       "      <td>es-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333</th>\n",
       "      <td>1045966224465768448</td>\n",
       "      <td>Mis tres gatos están con fiebre, mi madre y yo...</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>2018</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>es-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16468</th>\n",
       "      <td>1473039392809885697</td>\n",
       "      <td>@user @user Eso lo entiendo (aunque siendo el ...</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>2021</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>es-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16480</th>\n",
       "      <td>1473529806184755205</td>\n",
       "      <td>@user Lo siento ami Bella  . Pendiente con lo ...</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>2021</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>1</td>\n",
       "      <td>es-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16516</th>\n",
       "      <td>1473725280485318662</td>\n",
       "      <td>@user Si estoy de acuerdo en lo de la atención...</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>2021</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>es-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16536</th>\n",
       "      <td>1473947371159662592</td>\n",
       "      <td>@user Hemos acabado con la gripe, ha sido uno ...</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>2021</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>es-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>1477844836149514240</td>\n",
       "      <td>@user Pues la mitad estamos con gripe y la otr...</td>\n",
       "      <td>es</td>\n",
       "      <td>es</td>\n",
       "      <td>2022</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>es-es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                              tweet  \\\n",
       "3049   1041876687988948993  Yo sé que las mujeres tienen muchísimas aptitu...   \n",
       "3160   1044350928227364869  Hace como 1 mes que estoy con esta gripe del o...   \n",
       "3188   1044540627491475456  Que mañana vaya alguien de mi parte a currar, ...   \n",
       "3266   1045317807263617024                                 Tengo una gripe...   \n",
       "3333   1045966224465768448  Mis tres gatos están con fiebre, mi madre y yo...   \n",
       "...                    ...                                                ...   \n",
       "16468  1473039392809885697  @user @user Eso lo entiendo (aunque siendo el ...   \n",
       "16480  1473529806184755205  @user Lo siento ami Bella  . Pendiente con lo ...   \n",
       "16516  1473725280485318662  @user Si estoy de acuerdo en lo de la atención...   \n",
       "16536  1473947371159662592  @user Hemos acabado con la gripe, ha sido uno ...   \n",
       "16650  1477844836149514240  @user Pues la mitad estamos con gripe y la otr...   \n",
       "\n",
       "      target_lang lang  year                             final_annotation  \\\n",
       "3049           es   es  2018                      1. Likely ILI infection   \n",
       "3160           es   es  2018                      1. Likely ILI infection   \n",
       "3188           es   es  2018                      1. Likely ILI infection   \n",
       "3266           es   es  2018                      1. Likely ILI infection   \n",
       "3333           es   es  2018                      1. Likely ILI infection   \n",
       "...           ...  ...   ...                                          ...   \n",
       "16468          es   es  2021  3. Not Related to ILI or COVID-19 Infection   \n",
       "16480          es   es  2021                      1. Likely ILI infection   \n",
       "16516          es   es  2021  3. Not Related to ILI or COVID-19 Infection   \n",
       "16536          es   es  2021  3. Not Related to ILI or COVID-19 Infection   \n",
       "16650          es   es  2022                      1. Likely ILI infection   \n",
       "\n",
       "       overlap eval_lang  \n",
       "3049         0     es-es  \n",
       "3160         1     es-es  \n",
       "3188         0     es-es  \n",
       "3266         1     es-es  \n",
       "3333         0     es-es  \n",
       "...        ...       ...  \n",
       "16468        0     es-es  \n",
       "16480        1     es-es  \n",
       "16516        0     es-es  \n",
       "16536        0     es-es  \n",
       "16650        0     es-es  \n",
       "\n",
       "[163 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data[full_data.index.isin(fractional_split_idx.get('es_three')['valid_idx'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89f26182-aef1-42ca-bc5c-840f2e4f3c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['es_one', 'es_two', 'es_three'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractional_split_idx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15a3813a-f1dc-4415-a92b-018cbde42978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_idx', 'test_idx', 'valid_idx'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractional_split_idx.get('es_one').keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50dc92ec-d135-46a7-9a62-02156a20bac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gaueko0/users/nmishra/multiling_fludetection/final_evals/eval_revisedcateg/testset0.6_0.2_0.2/split_idx_es_learningcurve.json\n"
     ]
    }
   ],
   "source": [
    "# # save the split index\n",
    "print(split_path.joinpath(f'split_idx_{language}_learningcurve.json'))\n",
    "# with open(split_path.joinpath(f'split_idx_{language}_learningcurve.json'), 'w') as f:\n",
    "#     json.dump(fractional_split_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bde347de-0b1d-45c1-9ae5-15d5f762ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## 2 Original Translate Test\n",
    "# experiment_type = 'original_ttest'\n",
    "# translate_test = True\n",
    "# original_split_idx = {}\n",
    "\n",
    "# for lang, splits_dict in split_id.items():\n",
    "#     original_lang_split_idx = {}\n",
    "    \n",
    "#     for split_type, ids in splits_dict.items():                           # for each split in each language\n",
    "        \n",
    "#         if 'test' in split_type:                                          # if test\n",
    "#             sub_df = full_data[full_data['id'].isin(test_ids)]                # take all test ids\n",
    "#             sub_df = sub_df[sub_df['target_lang']==lang]                      # take only translations to that language\n",
    "#             sub_df = sub_df[sub_df['lang']!=sub_df['target_lang']]            # remove original as already added in test\n",
    "        \n",
    "#         else:                                                              # otherwise\n",
    "#             sub_df = full_data[full_data['id'].isin(ids)]                  # take split type ids for the language in loop only\n",
    "#             sub_df = sub_df[sub_df['lang']==sub_df['target_lang']]         # take only original\n",
    "        \n",
    "#         print(lang, split_type, sub_df.shape)\n",
    "#         original_lang_split_idx[split_type] = sub_df.index.tolist()        # save the index of the selected ids\n",
    "        \n",
    "#     original_split_idx[lang] = original_lang_split_idx\n",
    "#     print('-'*13)\n",
    "\n",
    "# print(original_split_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "155f747b-eef0-4559-9801-8d65507c5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check each index is uniquely placed in one split type of one language only\n",
    "# # test_set will have repeat here \n",
    "# all_split_types = original_split_idx.get('de').keys()\n",
    "# print(all_split_types)\n",
    "\n",
    "# for type_to_test in all_split_types:\n",
    "#     for i in range(21):\n",
    "#         n = np.random.randint(0, full_data.shape[0])\n",
    "#         test_df = pd.DataFrame.from_dict(original_split_idx)\n",
    "#         test_df = test_df[test_df.index==type_to_test]\n",
    "#         test_df = test_df[test_df.map(lambda x: n in x)]\n",
    "#         if (test_df.notna().sum().sum()==1) or test_df.notna().sum().sum()==0:\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(type_to_test)\n",
    "#             print(n)\n",
    "#             print('matched more than once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0c72576-85a2-4890-8a46-62f030dee6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_lang_split_idx = {}\n",
    "\n",
    "# original_df = full_data[full_data['lang']==full_data['target_lang']]\n",
    "# translated_df = full_data[full_data['lang']!=full_data['target_lang']]\n",
    "\n",
    "# original_lang_split_idx['train_idx'] = original_df[original_df['id'].isin(train_ids)].index.tolist()                # take all test ids\n",
    "# original_lang_split_idx['test_idx']  = translated_df[translated_df['id'].isin(test_ids)].index.tolist() \n",
    "# original_lang_split_idx['valid_idx'] = original_df[original_df['id'].isin(valid_ids)].index.tolist()\n",
    "# print([len(i) for i in original_lang_split_idx.values()])\n",
    "\n",
    "# original_split_idx['all'] = original_lang_split_idx\n",
    "# print(original_split_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a438de7a-1214-41ad-9637-9d0f5f48c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data[full_data.index.isin(original_split_idx['all']['test_idx'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ec871e3-3214-4d53-b295-ef1411b962d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the split index\n",
    "# print(split_path.joinpath(f'split_idx_{experiment_type}.json'))\n",
    "# with open(split_path.joinpath(f'split_idx_{experiment_type}.json'), 'w') as f:\n",
    "#     json.dump(original_split_idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bd6a9b-632d-4956-a358-866729799b0d",
   "metadata": {},
   "source": [
    "<a name=\"onetomany\"></a>\n",
    "\n",
    "## 3 One to Many\n",
    "\n",
    "### Get split index for each split of each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c90293b-9479-414b-94e7-64f85da08732",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_type = 'one_to_many'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca8f3d2d-76ac-438a-88f5-a7da0cd890c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de train_idx   2895\n",
      "de test_idx   4300\n",
      "de valid_idx   970\n",
      "en train_idx   585\n",
      "en test_idx   4300\n",
      "en valid_idx   195\n",
      "es train_idx   3490\n",
      "es test_idx   4300\n",
      "es valid_idx   1165\n",
      "fr train_idx   2880\n",
      "fr test_idx   4300\n",
      "fr valid_idx   960\n",
      "it train_idx   2985\n",
      "it test_idx   4300\n",
      "it valid_idx   995\n"
     ]
    }
   ],
   "source": [
    "onetomany_split_idx = {}\n",
    "for lang, splits_dict in split_id.items():\n",
    "    onetomany_lang_split_idx = {}\n",
    "    for split_type, ids in splits_dict.items():                           # for each split in each language\n",
    "        if 'test' in split_type:\n",
    "            sub_df = full_data[full_data['id'].isin(test_ids)]                # take all test ids\n",
    "            # sub_df = sub_df[sub_df['target_lang']==sub_df['lang']]            # take originals only\n",
    "            # onetomany_lang_split_idx[split_type] = sub_df.index.tolist()    # test on multilingual original \n",
    "            # print(lang, split_type, f'  {len(sub_df)}')\n",
    "        else:\n",
    "            sub_df = full_data[full_data['id'].isin(ids)]                     # take only id in the split_type for that langauge\n",
    "        print(lang, split_type, f'  {sub_df.shape[0]}')\n",
    "        # print(sub_df[['id','lang']].head())\n",
    "        onetomany_lang_split_idx[split_type] = sub_df.index.tolist() # get the index of the selected ids and save into respective split\n",
    "    onetomany_split_idx[lang] = onetomany_lang_split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ce82167-2e5d-48d4-9127-e3b7e1fa9668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_idx', 'test_idx', 'valid_idx'])\n",
      "test_idx\n",
      "7500\n",
      "matched more than once\n",
      "test_idx\n",
      "18310\n",
      "matched more than once\n",
      "test_idx\n",
      "13874\n",
      "matched more than once\n",
      "test_idx\n",
      "9683\n",
      "matched more than once\n"
     ]
    }
   ],
   "source": [
    "# check each index is uniquely placed in one split type of one language only\n",
    "# test_set will have repeat here \n",
    "all_split_types = onetomany_split_idx.get('de').keys()\n",
    "print(all_split_types)\n",
    "\n",
    "for type_to_test in all_split_types:\n",
    "    for i in range(21):\n",
    "        n = np.random.randint(0, full_data.shape[0])\n",
    "        test_df = pd.DataFrame.from_dict(onetomany_split_idx)\n",
    "        test_df = test_df[test_df.index==type_to_test]\n",
    "        test_df = test_df[test_df.map(lambda x: n in x)]\n",
    "        if (test_df.notna().sum().sum()==1) or test_df.notna().sum().sum()==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(type_to_test)\n",
    "            print(n)\n",
    "            print('matched more than once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db670830-ee12-4d20-a0f9-18c0c501b351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>target_lang</th>\n",
       "      <th>lang</th>\n",
       "      <th>year</th>\n",
       "      <th>final_annotation</th>\n",
       "      <th>overlap</th>\n",
       "      <th>eval_lang</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21850866789130240</td>\n",
       "      <td>Hola Twitter. En 2010 casi me muero de gripe, ...</td>\n",
       "      <td>es</td>\n",
       "      <td>fr</td>\n",
       "      <td>2011</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-es</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21850866789130240</td>\n",
       "      <td>Hallo Twitter. 2010 bin ich fast an einer Grip...</td>\n",
       "      <td>de</td>\n",
       "      <td>fr</td>\n",
       "      <td>2011</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-de</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21850866789130240</td>\n",
       "      <td>Hello Twitter. In 2010, I almost died of the f...</td>\n",
       "      <td>en</td>\n",
       "      <td>fr</td>\n",
       "      <td>2011</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-en</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21850866789130240</td>\n",
       "      <td>Ciao Twitter. Nel 2010 ho rischiato di morire ...</td>\n",
       "      <td>it</td>\n",
       "      <td>fr</td>\n",
       "      <td>2011</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-it</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21850866789130240</td>\n",
       "      <td>Bonjour Twitter. En 2010, j'ai frôlé la mort p...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>2011</td>\n",
       "      <td>1. Likely ILI infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-fr</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21070</th>\n",
       "      <td>1631134812764291078</td>\n",
       "      <td>@user @user @user On veut gérer le Covid comme...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>2023</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-fr</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21071</th>\n",
       "      <td>1631134812764291078</td>\n",
       "      <td>@user @user @user Vogliamo gestire la Covid co...</td>\n",
       "      <td>it</td>\n",
       "      <td>fr</td>\n",
       "      <td>2023</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-it</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21072</th>\n",
       "      <td>1631134812764291078</td>\n",
       "      <td>@usuario @usuario @usuario Queremos gestionar ...</td>\n",
       "      <td>es</td>\n",
       "      <td>fr</td>\n",
       "      <td>2023</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-es</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21073</th>\n",
       "      <td>1631134812764291078</td>\n",
       "      <td>@user @user @user Man will das Covid wie die G...</td>\n",
       "      <td>de</td>\n",
       "      <td>fr</td>\n",
       "      <td>2023</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-de</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21074</th>\n",
       "      <td>1631134812764291078</td>\n",
       "      <td>@user @user @user We want to manage Covid like...</td>\n",
       "      <td>en</td>\n",
       "      <td>fr</td>\n",
       "      <td>2023</td>\n",
       "      <td>3. Not Related to ILI or COVID-19 Infection</td>\n",
       "      <td>0</td>\n",
       "      <td>fr-en</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>960 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                              tweet  \\\n",
       "0        21850866789130240  Hola Twitter. En 2010 casi me muero de gripe, ...   \n",
       "1        21850866789130240  Hallo Twitter. 2010 bin ich fast an einer Grip...   \n",
       "2        21850866789130240  Hello Twitter. In 2010, I almost died of the f...   \n",
       "3        21850866789130240  Ciao Twitter. Nel 2010 ho rischiato di morire ...   \n",
       "4        21850866789130240  Bonjour Twitter. En 2010, j'ai frôlé la mort p...   \n",
       "...                    ...                                                ...   \n",
       "21070  1631134812764291078  @user @user @user On veut gérer le Covid comme...   \n",
       "21071  1631134812764291078  @user @user @user Vogliamo gestire la Covid co...   \n",
       "21072  1631134812764291078  @usuario @usuario @usuario Queremos gestionar ...   \n",
       "21073  1631134812764291078  @user @user @user Man will das Covid wie die G...   \n",
       "21074  1631134812764291078  @user @user @user We want to manage Covid like...   \n",
       "\n",
       "      target_lang lang  year                             final_annotation  \\\n",
       "0              es   fr  2011                      1. Likely ILI infection   \n",
       "1              de   fr  2011                      1. Likely ILI infection   \n",
       "2              en   fr  2011                      1. Likely ILI infection   \n",
       "3              it   fr  2011                      1. Likely ILI infection   \n",
       "4              fr   fr  2011                      1. Likely ILI infection   \n",
       "...           ...  ...   ...                                          ...   \n",
       "21070          fr   fr  2023  3. Not Related to ILI or COVID-19 Infection   \n",
       "21071          it   fr  2023  3. Not Related to ILI or COVID-19 Infection   \n",
       "21072          es   fr  2023  3. Not Related to ILI or COVID-19 Infection   \n",
       "21073          de   fr  2023  3. Not Related to ILI or COVID-19 Infection   \n",
       "21074          en   fr  2023  3. Not Related to ILI or COVID-19 Infection   \n",
       "\n",
       "       overlap eval_lang  original  \n",
       "0            0     fr-es     False  \n",
       "1            0     fr-de     False  \n",
       "2            0     fr-en     False  \n",
       "3            0     fr-it     False  \n",
       "4            0     fr-fr      True  \n",
       "...        ...       ...       ...  \n",
       "21070        0     fr-fr      True  \n",
       "21071        0     fr-it     False  \n",
       "21072        0     fr-es     False  \n",
       "21073        0     fr-de     False  \n",
       "21074        0     fr-en     False  \n",
       "\n",
       "[960 rows x 9 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_type = 'test_idx'\n",
    "split_type = 'train_idx'\n",
    "split_type = 'valid_idx'\n",
    "language = 'fr'\n",
    "full_data[full_data.index.isin(onetomany_split_idx[language][split_type])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "202d9cb5-afd3-48c6-8abc-cf81a2e5ac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gaueko0/users/nmishra/multiling_fludetection/final_evals/eval_revisedcateg/testset0.6_0.2_0.2/split_idx_one_to_many.json\n"
     ]
    }
   ],
   "source": [
    "# # save the split index\n",
    "print(split_path.joinpath(f'split_idx_{experiment_type}.json'))\n",
    "# with open(split_path.joinpath(f'split_idx_{experiment_type}.json'), 'w') as f:\n",
    "#     json.dump(onetomany_split_idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31ed57-7a21-4956-b75b-7516d09e3381",
   "metadata": {},
   "source": [
    "<a name=\"manytoone\"></a>\n",
    "\n",
    "## 4. Many to One\n",
    "### Get split index for each split of each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af276556-cc5b-476d-9641-7ee05983d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_type = 'many_to_one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3147bc9a-698b-475d-bfb2-300c31ced82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "(2567, 7) (857, 7) (860, 7)\n",
      "en\n",
      "(2567, 7) (857, 7) (860, 7)\n",
      "es\n",
      "(2567, 7) (857, 7) (860, 7)\n",
      "fr\n",
      "(2567, 7) (857, 7) (860, 7)\n",
      "it\n",
      "(2567, 7) (857, 7) (860, 7)\n",
      "[dict_keys(['train_idx', 'valid_idx', 'test_idx']), dict_keys(['train_idx', 'valid_idx', 'test_idx']), dict_keys(['train_idx', 'valid_idx', 'test_idx']), dict_keys(['train_idx', 'valid_idx', 'test_idx']), dict_keys(['train_idx', 'valid_idx', 'test_idx'])]\n"
     ]
    }
   ],
   "source": [
    "manytoone_split_idx = {}\n",
    "for lang, splits_dict in split_id.items():\n",
    "    print(lang)\n",
    "    sub_df = full_data[full_data['target_lang']==lang]\n",
    "    train = sub_df[sub_df['id'].isin(train_ids)]\n",
    "    valid = sub_df[sub_df['id'].isin(valid_ids)]\n",
    "    test = sub_df[sub_df['id'].isin(test_ids)]\n",
    "    # test_original = test[test['target_lang']==test['lang']]\n",
    "    print(train.shape, valid.shape, test.shape)\n",
    "    \n",
    "    manytoone_lang_split_idx = {}\n",
    "    manytoone_lang_split_idx['train_idx'] = train.index.tolist()         # train on monolingual original\n",
    "    manytoone_lang_split_idx['valid_idx'] = valid.index.tolist()         # valid on monolingual original\n",
    "    manytoone_lang_split_idx['test_idx'] = test.index.tolist()           # test on monolingual translated \n",
    "    manytoone_split_idx[lang] = manytoone_lang_split_idx\n",
    "print([j.keys() for i,j in manytoone_split_idx.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d40cad87-7a13-4f37-ab1e-eca9ce113054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_idx', 'valid_idx', 'test_idx'])\n"
     ]
    }
   ],
   "source": [
    "# check each index is uniquely placed in one split type of one language only\n",
    "# test_set will have repeat here \n",
    "all_split_types = manytoone_split_idx.get('de').keys()\n",
    "print(all_split_types)\n",
    "\n",
    "for type_to_test in all_split_types:\n",
    "    for i in range(21):\n",
    "        n = np.random.randint(0, full_data.shape[0])\n",
    "        test_df = pd.DataFrame.from_dict(manytoone_split_idx)\n",
    "        test_df = test_df[test_df.index==type_to_test]\n",
    "        test_df = test_df[test_df.map(lambda x: n in x)]\n",
    "        if (test_df.notna().sum().sum()==1) or test_df.notna().sum().sum()==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(type_to_test)\n",
    "            print(n)\n",
    "            print('matched more than once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b18571cb-c0d0-428e-aa90-6988a4330ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gaueko0/users/nmishra/multiling_fludetection/final_evals/eval_revisedcateg/testset0.6_0.2_0.2\n"
     ]
    }
   ],
   "source": [
    "# # save the split index\n",
    "print(split_path.joinpath(f'split_idx_{experiment_type}.json'))\n",
    "# with open(split_path.joinpath(f'split_idx_{experiment_type}.json'), 'w') as f:\n",
    "#     json.dump(manytoone_split_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750c101-062b-4b89-8e44-ca9dbcf8d649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel ILI Test",
   "language": "python",
   "name": "ipykernel-ili-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
