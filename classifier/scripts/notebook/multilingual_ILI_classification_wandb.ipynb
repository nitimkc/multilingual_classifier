{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920fb4e9-e23a-425c-b430-76acd2bb5c0d",
   "metadata": {},
   "source": [
    "WANDB ISSUES <br>\n",
    "https://github.com/wandb/wandb/issues/4534 <br>\n",
    "to delete runs before N hours. Note that N > duration time of the longest run, just to be make sure you do not delete an active run <br>\n",
    "\n",
    "`wandb sync --clean-old-hours N` <br>\n",
    "\n",
    "to not clean the online run folders. More details in our documentation here.https://docs.wandb.ai/ref/cli/wandb-sync <br>\n",
    "\n",
    "`--no-include-online` <br>\n",
    "\n",
    ".cache where artifact files are cached to boost performance <br>\n",
    "\n",
    "`wandb artifact cache cleanup 10GB` <br>\n",
    "https://docs.wandb.ai/ref/cli/wandb-artifact/wandb-artifact-cache/wandb-artifact-cache-cleanup <br>\n",
    "https://community.wandb.ai/t/how-to-speed-up-batch-delete-of-files-artifacts/4251/6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b6030-4b49-4485-bfdc-99f0bc7c7acd",
   "metadata": {},
   "source": [
    "WANDB directing files to /tmp via ipykernel <br>\n",
    "https://scipy-ipython.readthedocs.io/en/latest/install/kernel_install.html <br>\n",
    "\n",
    "To register ipykernel, first activate virtual environment, then install and then register <br>\n",
    "`python -m pip install ipykernel` <br>\n",
    "`python -m ipykernel install --name {MACHINE_NAME} --display-name \"{DISPLAY_NAME}\" --user`<br>\n",
    "`python -m ipykernel install --name=mari_kernel --user` <br>\n",
    "\n",
    "To remove kernel <br>\n",
    "`jupyter kernelspec uninstall {KERNEL_NAME}`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b3fc67-d8bd-41d6-8b42-f40333770782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# print(os.environ[\"TMPDIR\"])\n",
    "# print(os.environ[\"TEMP\"])\n",
    "# print(os.environ[\"TMP\"])\n",
    "\n",
    "# print(os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "# print(os.environ[\"HF_DATASETS_CACHE\"])\n",
    "# print(os.environ[\"HF_HOME\"])\n",
    "\n",
    "# KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c56667b-53b7-495b-bb0f-057cc645fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DIR\"] = \"/gaueko0/users/nmishra/multiling_fludetection/evals\"\n",
    "os.environ[\"WANDB_CACHE_DIR\"] = \"/gaueko0/users/nmishra/multiling_fludetection/evals\"\n",
    "os.environ[\"WANDB_CONFIG_DIR\"] = \"/gaueko0/users/nmishra/multiling_fludetection/evals\"\n",
    "os.environ[\"WANDB_ARTIFACT_DIR\"] = \"/gscratch3/users/nmishra/gridsearch_revisedcateg\"\n",
    "os.environ[\"WANDB_ARTIFACT_LOCATION\"] = \"/gscratch3/users/nmishra/gridsearch_revisedcateg\"\n",
    "os.environ[\"WANDB_DATA_DIR\"] = \"/gscratch3/users/nmishra/gridsearch_revisedcateg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd6ec47-1d0c-4026-bb20-676c0d2ba7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gaueko0/users/nmishra/multiling_fludetection/evals\n",
      "/gaueko0/users/nmishra/multiling_fludetection/evals\n",
      "/gaueko0/users/nmishra/multiling_fludetection/evals\n",
      "/gscratch3/users/nmishra/gridsearch_revisedcateg\n",
      "/gscratch3/users/nmishra/gridsearch_revisedcateg\n",
      "/gscratch3/users/nmishra/gridsearch_revisedcateg\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"WANDB_SILENT\"] = \"true\" # /home/my_user/code\n",
    "print(os.environ[\"WANDB_DIR\"]) # /gaueko0/users/nmishra/multiling_fludetection\n",
    "print(os.environ[\"WANDB_CACHE_DIR\"])\n",
    "print(os.environ[\"WANDB_CONFIG_DIR\"])\n",
    "print(os.environ[\"WANDB_ARTIFACT_DIR\"])\n",
    "print(os.environ[\"WANDB_ARTIFACT_LOCATION\"])\n",
    "print(os.environ[\"WANDB_DATA_DIR\"])\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #,5,6\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f307e8-9270-44fa-8f28-814aaedb66eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 28 11:14:07 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   23C    P0              59W / 500W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   56C    P0             279W / 500W |  72184MiB / 81920MiB |     95%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   46C    P0             266W / 500W |  72228MiB / 81920MiB |     95%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   22C    P0              61W / 500W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    1   N/A  N/A   3600322      C   ...nvironments/discharge_me/bin/python    72162MiB |\n",
      "|    2   N/A  N/A   3603646      C   ...nvironments/discharge_me/bin/python    72206MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "740109bb-3068-4d8f-bfc5-530a4a8c8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "1. Read annotated multilingual ILI data using CustomDataset.\n",
    "2. Convert encoded features and labels to dataset objects for integration with transformers model training.\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "    \n",
    "class CustomDataset(object):\n",
    "    def __init__(self, file_name, savepath=None):\n",
    "        \n",
    "        self._file_name = file_name\n",
    "        if savepath is not None:\n",
    "            self._savepath = Path(savepath)\n",
    "            self._savepath.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.data =  pd.read_csv(self._file_name)\n",
    "        self.tweets = self.data['tweet']\n",
    "        self.labels = self.data['final_annotation']\n",
    "\n",
    "    def __len__(self):    \n",
    "        if len(self.tweets) != len(self.labels):\n",
    "            raise sys.exit(f\"Number of tweets({len(self.tweets)}) and its labels({len(self.labels)}) do not match.\")\n",
    "        else:\n",
    "            return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets.iloc[idx] \n",
    "        label = self.labels.iloc[idx] \n",
    "        return tweet, label\n",
    "    \n",
    "    def getsplitidx(self, test_split=0.2, valid_split=None, group='lang', stratify_label='final_annotation'):\n",
    "        \n",
    "        # group day by language and then perform stratified split by categories and save indices as json\n",
    "        lang_split_idx = {}\n",
    "        for grp, grp_df in self.data.groupby(group): \n",
    "            train, test = train_test_split(grp_df, test_size=test_split, stratify=grp_df[stratify_label])\n",
    "            if valid_split is not None:\n",
    "                train, valid = train_test_split(train, test_size=valid_split, stratify=train[stratify_label])\n",
    "        \n",
    "            print(f\"\\nDistribution of classes in train set in {grp}\\n{train[stratify_label].value_counts()}\")\n",
    "            print(f\"Distribution of classes in test set in {grp}\\n\\{test[stratify_label].value_counts()}\")\n",
    "            lang_split_idx[grp] = {'train_idx':train.index.values.tolist(), \n",
    "                                   'test_idx':test.index.values.tolist()\n",
    "                                  }\n",
    "            if valid_split is not None:\n",
    "                print(f\"Distribution of classes in valid set in {grp}\\n{valid[stratify_label].value_counts()}\\n\")\n",
    "                lang_split_idx[grp] = {'train_idx':train.index.values.tolist(), \n",
    "                                       'test_idx':test.index.values.tolist(), \n",
    "                                       'valid_idx':valid.index.values.tolist()\n",
    "                                      }\n",
    "                \n",
    "        if self._savepath is not None:\n",
    "            with open(self._savepath.joinpath(\"lang_split_idx.json\"), \"w\")  as f:\n",
    "                json.dump(lang_split_idx, f)\n",
    "        return lang_split_idx\n",
    "\n",
    "# https://huggingface.co/transformers/v3.5.1/custom_datasets.html    \n",
    "class EncodedDataset(torch.utils.data.Dataset): # torch\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = {key: torch.tensor(val[idx]).clone().detach() for key, val in self.encodings.items()}\n",
    "        # item['labels'] = torch.tensor(self.labels[idx]).clone().detach()\n",
    "        \n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getdatasetsplits__(self, indices):\n",
    "        split = self.__getitem__(indices)\n",
    "        dataset = Dataset.from_dict(split)\n",
    "        return dataset\n",
    "    \n",
    "    def splitdata(self, split_indices):\n",
    "        allsplits = []\n",
    "        for i in split_indices:\n",
    "            allsplits.append(self.__getdatasetsplits__(i))\n",
    "        return allsplits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c315cb90-080b-4d95-91d7-8edfd01601c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "1. Process data based on model checkpoint and configurations provided in final_configs.json \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \n",
    "    def __init__(self, config, encoder=LabelEncoder(), return_type_ids=False):\n",
    "        \n",
    "        self._config = config\n",
    "        self._encoder = encoder\n",
    "        self._return_type_ids = return_type_ids\n",
    "        \n",
    "        self.model_checkpoint = self._config['MODEL_CHECKPOINT']\n",
    "        if ((self._config['MAX_LEN'] is not None and self._config['MAX_LEN']>128) and ('bernice' in self.model_checkpoint)):\n",
    "            self._config['MAX_LEN'] = 128\n",
    "            print(f\"Max length for {self.model_checkpoint} set to 128 by default\")\n",
    "        print(f\"Final configurations for processing training + validation data\\n{self._config}\")\n",
    "\n",
    "    def label_encoder(self, target):\n",
    "        le = self._encoder\n",
    "        return le.fit_transform(target)\n",
    "\n",
    "    def tokenizer(self):    \n",
    "        # statistical tokenizer # subwords, chunks of words \n",
    "        return AutoTokenizer.from_pretrained(self.model_checkpoint, \n",
    "                                             use_fast = False,    # use one of the fast tokenizers (backed by Rust), available for almost all models\n",
    "                                             # max_length=self._config['MAX_LEN'] # pass max length only when encoding not when instantiating the tokenizer\n",
    "                                             )\n",
    "    \n",
    "    def feature_encoder(self, features):\n",
    "        tokenizer = self.tokenizer()\n",
    "\n",
    "        feature_encodings = tokenizer.batch_encode_plus(\n",
    "            features.astype(str).values.tolist(), \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=self._config['MAX_LEN'],\n",
    "            # is_split_into_words=True, # added for multilingual versions refer 4624.err\n",
    "            # return_attention_mask=True,\n",
    "            return_token_type_ids=self._return_type_ids, \n",
    "            return_tensors='pt',\n",
    "            )\n",
    "        print(f\"Dimensions of encoded features: {feature_encodings['input_ids'].shape}\")\n",
    "        print(f\"Encoding contains: {[i for i in feature_encodings.keys()]}\")\n",
    "        return feature_encodings\n",
    "\n",
    "    def encoded_data(self, features, labels):\n",
    "        encoded_features = self.feature_encoder(features)\n",
    "        encoded_labels = self.label_encoder(labels)\n",
    "        if encoded_features['input_ids'].shape[0] == encoded_labels.shape[0]:\n",
    "            return encoded_features, encoded_labels\n",
    "        else:\n",
    "            print(\"encoded features and labels do not have same length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab5e80e-fb37-49bf-9816-de5d35b36c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import EarlyStoppingCallback, TrainerCallback\n",
    "\n",
    "import evaluate\n",
    "# from evaluater import evaluation_display\n",
    "\n",
    "import wandb\n",
    "\n",
    "def compute_metrics(eval_pred, eval_metric=\"f1\"):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    metric = evaluate.load(eval_metric)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    # precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    # acc = accuracy_score(labels, predictions)\n",
    "    # return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "    # accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    # f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    # return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "class ModelTrainer(object):\n",
    "\n",
    "    def __init__(self, model_checkpoint, train_dataset, valid_dataset, test_dataset, \n",
    "                 tokenizer, savepath, cachepath, target_names=None, lang_to_train='all'):     \n",
    "        \n",
    "        self._model_checkpoint = model_checkpoint\n",
    "        self.model_name = self._model_checkpoint.split(\"/\")[-1]\n",
    "        \n",
    "        self._train_dataset = train_dataset\n",
    "        self._valid_dataset = valid_dataset\n",
    "        \n",
    "        self._tokenizer = tokenizer\n",
    "        \n",
    "        self._cachepath = cachepath#.joinpath('.cache')\n",
    "        self._savepath = savepath\n",
    "        self.modelpath = self._savepath.joinpath('models')\n",
    "        self.modelpath.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        \n",
    "        self._target_names = target_names\n",
    "        self.num_labels = len(self._target_names)\n",
    "        \n",
    "        self._lang_to_train = lang_to_train\n",
    "        \n",
    "    def get_model(self):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self._model_checkpoint,\n",
    "                                                                   num_labels=self.num_labels,\n",
    "                                                                   cache_dir=self._cachepath,\n",
    "                                                                #    output_attentions=False,\n",
    "                                                                #    output_hidden_states=False,\n",
    "                                                                #    ignore_mismatched_sizes=True,\n",
    "                                                                )\n",
    "        # wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "        # print(model.config)\n",
    "        return model\n",
    "    \n",
    "    def train_eval(self, config=None, metric_name=\"f1\"):\n",
    "        out_dir = self.modelpath.joinpath(f\"{self.model_name}-{self._lang_to_train}-finetuned\")\n",
    "        log_dir = self.modelpath.parent.joinpath('logs').joinpath(f\"{self.model_name}-{self._lang_to_train}-finetuned\")\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # attributes to customize the training\n",
    "        with wandb.init(config=config):\n",
    "            config = wandb.config\n",
    "            print(f\"\\nTraining {self.model_name} using with configurations:\\n{config}\")\n",
    "            \n",
    "            # https://discuss.huggingface.co/t/cuda-out-of-memory-when-using-trainer-with-compute-metrics/2941/22\n",
    "            args = TrainingArguments(\n",
    "                save_total_limit = 2,\n",
    "                output_dir = str(out_dir),\n",
    "                overwrite_output_dir = True,\n",
    "                logging_dir = str(log_dir),\n",
    "                \n",
    "                learning_rate = config.learning_rate,\n",
    "                per_device_train_batch_size = config.batch_size,\n",
    "                per_device_eval_batch_size = config.batch_size,\n",
    "                num_train_epochs = config.epochs,\n",
    "                # weight_decay = config.weight_decay,\n",
    "                \n",
    "                evaluation_strategy = \"epoch\",\n",
    "                save_strategy = \"epoch\",  \n",
    "                logging_strategy='epoch', \n",
    "\n",
    "                eval_accumulation_steps=1,\n",
    "                metric_for_best_model = metric_name,\n",
    "                push_to_hub=False, # push the model to the Hub regularly during training\n",
    "                load_best_model_at_end = True,\n",
    "                # report_to='wandb',  # turn on wandb logging\n",
    "                )\n",
    "        \n",
    "        # https://huggingface.co/docs/transformers/main_classes/trainer#trainer\n",
    "        trainer = Trainer(\n",
    "            model_init = self.get_model,\n",
    "            args = args,\n",
    "            train_dataset = self._train_dataset,\n",
    "            eval_dataset = self._valid_dataset,\n",
    "            tokenizer = self._tokenizer,\n",
    "            compute_metrics = compute_metrics,\n",
    "            callbacks = [EarlyStoppingCallback(3, 0.0)]\n",
    "            )\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        # print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "        \n",
    "        trainer.train()\n",
    "        trainer.evaluate()\n",
    "        \n",
    "        print(f\"free space by deleting: {out_dir}\")\n",
    "        shutil.rmtree(out_dir, ignore_errors=True)\n",
    "\n",
    "# mytrainer = trainer.train_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd138ab-68cd-4d4d-9973-0adac7f49be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "The script for wrapper function to run multilingual_ILI_classification_wandb.py\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import shutil\n",
    "import logging as log\n",
    "import wandb \n",
    "\n",
    "# from reader import EncodedDataset\n",
    "# from preprocessor import DataProcessor\n",
    "# from trainer_wandb import ModelTrainer\n",
    "\n",
    "def getsplit(lang_split_idx, key='train_idx'):\n",
    "    idx_list = [v[key] for k,v in lang_split_idx.items()]\n",
    "    # print(len(idx_list))\n",
    "    idx_list = [i for eachlist in idx_list for i in eachlist]\n",
    "    return idx_list\n",
    "    \n",
    "def mlm_evaluation(lang_split_idx, tweets, params, sweep_config, split_path, dirname, lang_to_train, cache_path, \n",
    "                   hyperparams=None, save=False, n_searches=5):\n",
    "\n",
    "    # get train, valid and test split for selected languages\n",
    "    train_idx = getsplit(lang_split_idx, key='train_idx')\n",
    "    valid_idx = getsplit(lang_split_idx, key='valid_idx')\n",
    "    test_idx = getsplit(lang_split_idx, key='test_idx')\n",
    "    print(f\"Distribution of data in train, validation and test splits: {len(train_idx)}, {len(valid_idx)}, {len(test_idx)}\")\n",
    "\n",
    "    # save test set for evaluation later\n",
    "    if save:\n",
    "        test_df = tweets.data.iloc[test_idx]\n",
    "        test_df.rename_axis('index').to_csv(split_path.joinpath(f\"{dirname}_{lang_to_train}.csv\"))\n",
    "\n",
    "    for model_checkpoint in params['MODEL_CHECKPOINT']:\n",
    "        # print(model_checkpoint)\n",
    "        model_params = {'MODEL_CHECKPOINT':model_checkpoint, \n",
    "                        'MAX_LEN':params['MAX_LEN'],\n",
    "                        'TARGET_NAMES':params['TARGET_NAMES']\n",
    "                       } \n",
    "        \n",
    "        # convert data to encoded features and labels\n",
    "        start_time = time.time()\n",
    "        processor = DataProcessor(model_params, return_type_ids=True)\n",
    "        feature_encodings, label_encodings = processor.encoded_data(tweets.data['tweet'], tweets.data['final_annotation'])\n",
    "\n",
    "        # obtain encoded train, valid and test data as dataset object\n",
    "        encoded_dataset = EncodedDataset(feature_encodings, label_encodings)\n",
    "        train_dataset, valid_dataset, test_dataset = encoded_dataset.splitdata([train_idx, valid_idx, test_idx])\n",
    "        \n",
    "        try:     \n",
    "            # train with hyperparameter provided    \n",
    "            trainer = ModelTrainer(model_checkpoint, train_dataset, valid_dataset, test_dataset, processor.tokenizer(), \n",
    "                                   split_path, cache_path, model_params['TARGET_NAMES'], lang_to_train)\n",
    "            sweep_id = wandb.sweep(sweep=sweep_config, entity=sweep_config['entity'], project=sweep_config['project'])\n",
    "            wandb.agent(sweep_id, trainer.train_eval, count=n_searches)\n",
    "            wandb.finish()  \n",
    "            print(f\"COMPLETED - {trainer.model_name} trained on {lang_to_train} languages\")\n",
    "\n",
    "            # delete wandb folder related to this model checkpoint\n",
    "            print(f\"free space by deleting: {split_path.parent.joinpath('wandb')}\")\n",
    "            shutil.rmtree(split_path.parent.joinpath('wandb'), ignore_errors=True)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(\"An error occurred:\", error)\n",
    "            log.exception('Failed')\n",
    "            pass \n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Total execution time to finetune {trainer.model_name} on {lang_to_train} language(s) is {execution_time}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8fe7ddd-05d7-4a1d-88b7-594ea9c0d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb artifacts and data saved in \n",
      "/gscratch3/users/nmishra/gridsearch_revisedcateg/gridsearch_revisedcateg \n",
      "and rest in /gaueko0/users/nmishra/multiling_fludetection/evals/gridsearch_revisedcateg\n",
      "\n",
      "Number of tweets in data: 4284\n",
      "Distribution of classes in all data final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection    2587\n",
      "1. Likely ILI infection                        1697\n",
      "Name: count, dtype: int64\n",
      "Configuration setup: {'MODEL_CHECKPOINT': ['jhu-clsp/bernice', 'cardiffnlp/twitter-xlm-roberta-base', 'bert-base-multilingual-uncased', 'microsoft/mdeberta-v3-base', 'xlm-roberta-base'], 'LANG': 'all', 'BATCH_SIZE': 16, 'LEARNING_RATE': 3.74e-05, 'EPOCHS': 6, 'MAX_LEN': 128, 'SPLITS': [[0.6, 0.2, 0.2]], 'TARGET_NAMES': ['1. Likely ILI infection', '3. Not Related to ILI or COVID-19 Infection']}\n",
      "\n",
      "Configuration setup for gridsearch: {'learning_rate': {'distribution': 'log_uniform_values', 'min': 1e-05, 'max': 0.1}, 'batch_size': {'values': [8, 16, 32]}, 'epochs': {'value': 1}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "The script to run to perform gridsearch using wandb\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import logging as log\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# from reader import CustomDataset, EncodedDataset\n",
    "# from preprocessor import DataProcessor\n",
    "# from trainer import hyperparams, ModelTrainer\n",
    "# from evaluater import PredictionEvaluater\n",
    "# from wandb_wrapper import mlm_evaluation\n",
    "\n",
    "# args\n",
    "DATA_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/data/all/alldata_revised.csv\")\n",
    "PARAMS_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/params.json\")\n",
    "CONFIG_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/sweep_config.yml\")\n",
    "OUT_PATH = Path(\"/gaueko0/users/nmishra/multiling_fludetection/evals/gridsearch_revisedcateg\")\n",
    "WANDB_DATA_PATH = Path(\"/gscratch3/users/nmishra/gridsearch_revisedcateg\")\n",
    "\n",
    "load_dotenv()\n",
    "# wandb.login()\n",
    "\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = str(OUT_PATH.joinpath(\".cache\"))\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = str(OUT_PATH.joinpath(\".cache\"))\n",
    "# os.environ['HF_HOME'] = str(OUT_PATH.joinpath(\".cache\"))\n",
    "# os.environ[\"WANDB_CACHE_DIR\"] = str(OUT_PATH.joinpath(\".cache\"))\n",
    "\n",
    "# ensure outpath exists if not create\n",
    "OUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WANDB_DATA_PATH = str(WANDB_DATA_PATH.joinpath(OUT_PATH.stem))\n",
    "os.environ[\"WANDB_DIR\"] = os.environ[\"WANDB_CACHE_DIR\"] = os.environ[\"WANDB_CONFIG_DIR\"] = str(OUT_PATH)\n",
    "os.environ[\"WANDB_ARTIFACT_DIR\"] = os.environ[\"WANDB_ARTIFACT_LOCATION\"] = os.environ[\"WANDB_DATA_DIR\"] = WANDB_DATA_PATH \n",
    "print(f\"wandb artifacts and data saved in \\n{WANDB_DATA_PATH } \\nand rest in {OUT_PATH}\\n\")\n",
    "\n",
    "# read data\n",
    "data_path = DATA_FILE.parent\n",
    "tweets = CustomDataset(DATA_FILE, data_path)\n",
    "print(f\"Number of tweets in data: {tweets.__len__()}\")\n",
    "print(f\"Distribution of classes in all data {tweets.labels.value_counts()}\")\n",
    "\n",
    "with open (PARAMS_FILE, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "params['MAX_LEN'] = None if params['MAX_LEN']=='None' else params['MAX_LEN']\n",
    "params['TARGET_NAMES'] = np.unique(tweets.labels).tolist()\n",
    "print(f\"Configuration setup: {params}\\n\")  \n",
    "\n",
    "with open (CONFIG_FILE, \"r\") as f:\n",
    "    sweep_config = yaml.safe_load(f)\n",
    "print(f\"Configuration setup for gridsearch: {sweep_config['parameters']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0267256d-1792-4973-b002-d44c8cde17c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for split in params['SPLITS']:\n",
    "\n",
    "    # cache dir\n",
    "    cache_path = OUT_PATH.parent.joinpath('.cache')\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Cache in {cache_path}\")\n",
    "\n",
    "    # data split index info\n",
    "    dirname = f\"testset{'_'.join([str(i) for i in split])}\"\n",
    "    split_path = OUT_PATH.joinpath(dirname)\n",
    "    print(f\"Reading data split index from: {split_path}\")\n",
    "    with open(split_path.joinpath('split_idx.json'), 'r') as f:\n",
    "        split_idx = json.load(f) \n",
    "        \n",
    "    # determine languages for which to get split index\n",
    "    if params['LANG']=='all':\n",
    "        languages = [i for i in split_idx]\n",
    "    else:\n",
    "        languages = [i for i in split_idx if i in params['LANG'].split(',')]\n",
    "    \n",
    "    # train on all languages and then on each language\n",
    "    lang_split_idx = {i:split_idx[i] for i in languages}\n",
    "    print(f\"Training data used for {params['LANG']} languages\")\n",
    "    mlm_evaluation(lang_split_idx, tweets, params, sweep_config, split_path, dirname, params['LANG'], cache_path)\n",
    "    \n",
    "    for lang_to_train in languages:\n",
    "        #if lang_to_train=='fr' or lang_to_train=='it':\n",
    "        print(f\"\\nTraining data used for {lang_to_train} language\")\n",
    "        lang_split_idx = {}\n",
    "        lang_split_idx[lang_to_train] = split_idx[lang_to_train]\n",
    "        mlm_evaluation(lang_split_idx, tweets, params, sweep_config, split_path, dirname, lang_to_train, cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0e41f7e-2cc4-40f6-bcb6-6c76ac3528f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-PCIE-32GB\n",
      "1\n",
      "Allocated Memory: 6.2 GB\n",
      "Cached Memory:    9.2 GB\n",
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(torch.cuda.get_device_name())\n",
    "    print(torch.cuda.device_count())\n",
    "    print('Allocated Memory:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached Memory:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348649aa-32d9-4ec8-979b-7a693702320e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel ILI Test",
   "language": "python",
   "name": "ipykernel-ili-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
