{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e58c69-7643-41ba-aa8c-9e8f5f656ca5",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Run Setting](#runsettings)\n",
    "2. [Reader](#reader)\n",
    "3. [Processor](#processor)\n",
    "4. [Trainer](#trainer)\n",
    "5. [Evaluater](#evaluater)\n",
    "6. [Wrapper](#wrapper)\n",
    "7. [Load Data](#loaddata)\n",
    "8. [Do Training](#dotrianing)\n",
    "\n",
    "## Configurations to run the script\n",
    "<a name=\"runsettings\"></a>\n",
    "\n",
    "- THIS RUN IS A RESULT OF TRANING USING BEST COMBINATION OF HYPERPARAMETERS\n",
    "- THE DATA HAS BEEN ENCODED BY TAKING THE ENTIRE DATA FOR ALL OR PER LANG (TRAIN+VALID+TEST)\n",
    "- HYPERPARAMETERS ARE OBTAINED BY TRANING WITH DATA ENCODED THROUGH THE ENTIRE DATA FOR ALL LANGUAGES\n",
    "- ALL & EACH\n",
    "- RESULTS in 8927.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c60089-6388-45d8-92b3-db352e204982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B disabled.\n"
     ]
    }
   ],
   "source": [
    "!wandb disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b38a1f-9115-4899-bb08-88248fe2d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2a8543-123e-4ac3-921d-5855f10f549b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_LAUNCH_BLOCKING=1: Command not found.\n"
     ]
    }
   ],
   "source": [
    "! CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92e7041-b0c2-4aaf-b5a6-a4e933628a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar  8 14:44:57 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.98                 Driver Version: 535.98       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              24W / 165W |     18MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A30                     Off | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              26W / 165W |     21MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2750731      G   /usr/libexec/Xorg                             4MiB |\n",
      "|    1   N/A  N/A   2750731      G   /usr/libexec/Xorg                             4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a4b66-bf79-4a6b-9cff-cd2140d9e2a9",
   "metadata": {},
   "source": [
    "## Reader \n",
    "<a name=\"reader\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5584c8d-752a-420f-846b-00dd66b0d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "1. Read annotated multilingual ILI data using CustomDataset.\n",
    "2. Convert encoded features and labels to dataset objects for integration with transformers model training.\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "    \n",
    "class CustomDataset(object):\n",
    "    def __init__(self, file_name, savepath=None):\n",
    "        \n",
    "        self._file_name = file_name\n",
    "        if savepath is not None:\n",
    "            self._savepath = Path(savepath)\n",
    "            self._savepath.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.data =  pd.read_csv(self._file_name)\n",
    "        self.tweets = self.data['tweet']\n",
    "        self.labels = self.data['final_annotation']\n",
    "\n",
    "    def __len__(self):    \n",
    "        if len(self.tweets) != len(self.labels):\n",
    "            raise sys.exit(f\"Number of tweets({len(self.tweets)}) and its labels({len(self.labels)}) do not match.\")\n",
    "        else:\n",
    "            return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweets.iloc[idx] \n",
    "        label = self.labels.iloc[idx] \n",
    "        return tweet, label\n",
    "    \n",
    "    def getsplitidx(self, test_split=0.2, valid_split=None, group='lang', stratify_label='final_annotation'):\n",
    "        \n",
    "        # group day by language and then perform stratified split by categories and save indices as json\n",
    "        lang_split_idx = {}\n",
    "        for grp, grp_df in self.data.groupby(group): \n",
    "            train, test = train_test_split(grp_df, test_size=test_split, stratify=grp_df[stratify_label])\n",
    "            if valid_split is not None:\n",
    "                train, valid = train_test_split(train, test_size=valid_split, stratify=train[stratify_label])\n",
    "        \n",
    "            print(f\"\\nDistribution of classes in train set in {grp}\\n{train[stratify_label].value_counts()}\")\n",
    "            print(f\"Distribution of classes in test set in {grp}\\n\\{test[stratify_label].value_counts()}\")\n",
    "            lang_split_idx[grp] = {'train_idx':train.index.values.tolist(), \n",
    "                                   'test_idx':test.index.values.tolist()\n",
    "                                  }\n",
    "            if valid_split is not None:\n",
    "                print(f\"Distribution of classes in valid set in {grp}\\n{valid[stratify_label].value_counts()}\\n\")\n",
    "                lang_split_idx[grp] = {'train_idx':train.index.values.tolist(), \n",
    "                                       'test_idx':test.index.values.tolist(), \n",
    "                                       'valid_idx':valid.index.values.tolist()\n",
    "                                      }\n",
    "                \n",
    "        if self._savepath is not None:\n",
    "            with open(self._savepath.joinpath(\"lang_split_idx.json\"), \"w\")  as f:\n",
    "                json.dump(lang_split_idx, f)\n",
    "        return lang_split_idx\n",
    "\n",
    "# https://huggingface.co/transformers/v3.5.1/custom_datasets.html    \n",
    "class EncodedDataset(torch.utils.data.Dataset): # torch\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = {key: torch.tensor(val[idx]).clone().detach() for key, val in self.encodings.items()}\n",
    "        # item['labels'] = torch.tensor(self.labels[idx]).clone().detach()\n",
    "        \n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getdatasetsplits__(self, indices):\n",
    "        split = self.__getitem__(indices)\n",
    "        dataset = Dataset.from_dict(split)\n",
    "        return dataset\n",
    "    \n",
    "    def splitdata(self, split_indices):\n",
    "        allsplits = []\n",
    "        for i in split_indices:\n",
    "            allsplits.append(self.__getdatasetsplits__(i))\n",
    "        return allsplits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b5eaa-a664-469c-a5a7-16c62386506a",
   "metadata": {},
   "source": [
    "## Preprocessor \n",
    "<a name=\"processor\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aae53cc-845c-48ab-ad04-cbce028651c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "1. Process data based on model checkpoint and configurations provided in final_configs.json \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \n",
    "    def __init__(self, config, encoder=LabelEncoder(), return_type_ids=False):\n",
    "        \n",
    "        self._config = config\n",
    "        self._encoder = encoder\n",
    "        self._return_type_ids = return_type_ids\n",
    "        \n",
    "        self.model_checkpoint = self._config['MODEL_CHECKPOINT']\n",
    "        if ((self._config['MAX_LEN'] is not None and self._config['MAX_LEN']>128) and ('bernice' in self.model_checkpoint)):\n",
    "            self._config['MAX_LEN'] = 128\n",
    "            print(f\"Max length for {self.model_checkpoint} set to 128 by default\")\n",
    "        print(f\"\\nFinal configurations for processing training + validation data\\n{self._config}\")\n",
    "\n",
    "    def label_encoder(self, target):\n",
    "        le = self._encoder\n",
    "        return le.fit_transform(target)\n",
    "\n",
    "    def tokenizer(self):    \n",
    "        # statistical tokenizer # subwords, chunks of words \n",
    "        return AutoTokenizer.from_pretrained(self.model_checkpoint, \n",
    "                                             use_fast = False,    # use one of the fast tokenizers (backed by Rust), available for almost all models\n",
    "                                             # max_length=self._config['MAX_LEN'] # pass max length only when encoding not when instantiating the tokenizer\n",
    "                                             )\n",
    "    \n",
    "    def feature_encoder(self, features):\n",
    "        tokenizer = self.tokenizer()\n",
    "\n",
    "        feature_encodings = tokenizer.batch_encode_plus(\n",
    "            features.astype(str).values.tolist(), \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=self._config['MAX_LEN'],\n",
    "            # is_split_into_words=True, # added for multilingual versions refer 4624.err\n",
    "            # return_attention_mask=True,\n",
    "            return_token_type_ids=self._return_type_ids, \n",
    "            return_tensors='pt',\n",
    "            )\n",
    "        print(f\"Dimensions of encoded features: {feature_encodings['input_ids'].shape}\")\n",
    "        print(f\"Encoding contains: {[i for i in feature_encodings.keys()]}\")\n",
    "        return feature_encodings\n",
    "\n",
    "    def encoded_data(self, features, labels):\n",
    "        encoded_features = self.feature_encoder(features)\n",
    "        encoded_labels = self.label_encoder(labels)\n",
    "        if encoded_features['input_ids'].shape[0] == encoded_labels.shape[0]:\n",
    "            return encoded_features, encoded_labels\n",
    "        else:\n",
    "            print(\"encoded features and labels do not have same length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6eee10-5b1b-4542-9905-607447e784ee",
   "metadata": {},
   "source": [
    "## Trainer \n",
    "<a name=\"trainer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53d368f-08d2-4143-aa67-b9cbd4ee603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "1. Train model once the best hyperparameters from final_configs.json that were identified using classification_wandb.py\n",
    "2. Get predictions from the trained model with boolen flag\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import EarlyStoppingCallback, TrainerCallback\n",
    "\n",
    "import evaluate\n",
    "# from evaluater import evaluation_display\n",
    "\n",
    "hyperparams = ['MODEL_CHECKPOINT','BATCH_SIZE','LEARNING_RATE','EPOCHS','MAX_LEN']\n",
    "\n",
    "# class PrintClassificationCallback(TrainerCallback):\n",
    "#     def on_evaluate(self, args, state, control, logs=None, **kwargs):\n",
    "#         print(\"Called after evaluation phase\")\n",
    "\n",
    "class ModelTrainer(object):\n",
    "\n",
    "    def __init__(self, run_config, train_dataset, valid_dataset, test_dataset, \n",
    "                 tokenizer, savepath, cachepath, lang_to_train='all'):     \n",
    "\n",
    "        self._run_config = run_config\n",
    "        if not all(k in self._run_config.keys() for k in hyperparams):\n",
    "            sys.exit(f\"provide all required hyperparams: {hyperparams}\")\n",
    "            \n",
    "        self.model_checkpoint = self._run_config['MODEL_CHECKPOINT']\n",
    "        self.model_name = self.model_checkpoint.split('/')[-1]\n",
    "    \n",
    "        self._train_dataset = train_dataset\n",
    "        self._valid_dataset = valid_dataset\n",
    "        self._test_dataset = test_dataset\n",
    "        self._train_dataset.cleanup_cache_files()\n",
    "\n",
    "        self._tokenizer = tokenizer\n",
    "        \n",
    "        self._cachepath = cachepath\n",
    "        self.modelpath = savepath.joinpath('models')\n",
    "        self.modelpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self._lang_to_train = lang_to_train\n",
    "        self.num_labels = len(self._run_config['TARGET_NAMES'])\n",
    "        print(f\"{self.num_labels} classes in {self._lang_to_train} language\")\n",
    "\n",
    "    def get_model(self):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.model_checkpoint,\n",
    "                                                                   num_labels = self.num_labels,\n",
    "                                                                   cache_dir = self._cachepath,\n",
    "                                                                #    output_attentions=False,\n",
    "                                                                #    output_hidden_states=False,\n",
    "                                                                #    ignore_mismatched_sizes=True,\n",
    "                                                                )\n",
    "        # print(model)\n",
    "        return model\n",
    "        \n",
    "    def compute_metrics(self, eval_pred, eval_metric=\"f1\"):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        # evaluation_display(labels, predictions, self.num_labels, self._target_names) # print only for now\n",
    "        metric = evaluate.load(eval_metric)\n",
    "        return metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    def train_eval(self, get_pred=False, metric_name=\"f1\", hyperparam_search=False):\n",
    "        \n",
    "        out_dir = self.modelpath.joinpath(f\"{self.model_name}-{self._lang_to_train}-finetuned\")\n",
    "        print(f\"Model to be saved in {out_dir}\")\n",
    "        log_dir = self.modelpath.parent.joinpath('logs').joinpath(f\"{self.model_name}-{self._lang_to_train}-finetuned\")\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        config = SimpleNamespace(**{i.lower():j for i,j in self._run_config.items() if i in hyperparams})\n",
    "        print(f\"\\nFinal configurations for training the model:\\n{config}\")\n",
    "        \n",
    "        # attributes to customize the training\n",
    "        args = TrainingArguments(\n",
    "            save_total_limit=2,\n",
    "            output_dir=str(out_dir),\n",
    "            overwrite_output_dir = True,\n",
    "            \n",
    "            learning_rate = config.learning_rate,\n",
    "            per_device_train_batch_size = config.batch_size,\n",
    "            per_device_eval_batch_size = config.batch_size,\n",
    "            num_train_epochs = config.epochs,\n",
    "            # weight_decay = config.weight_decay,\n",
    "            \n",
    "            evaluation_strategy = \"epoch\",\n",
    "            save_strategy = \"epoch\",   \n",
    "            logging_strategy = 'epoch',\n",
    "            \n",
    "            logging_steps= 1,\n",
    "            eval_accumulation_steps = 1,\n",
    "            \n",
    "            metric_for_best_model = metric_name,\n",
    "            load_best_model_at_end = True,\n",
    "            \n",
    "            push_to_hub = False, # push the model to the Hub regularly during training\n",
    "            # report_to='wandb',  # turn on wandb logging\n",
    "            )\n",
    "\n",
    "        # https://huggingface.co/docs/transformers/main_classes/trainer#trainer\n",
    "        # https://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/trainer.py#L231\n",
    "        trainer = Trainer(\n",
    "            model_init = self.get_model,\n",
    "            args = args,\n",
    "            train_dataset = self._train_dataset,\n",
    "            eval_dataset = self._valid_dataset,\n",
    "            tokenizer = self._tokenizer,\n",
    "            compute_metrics = self.compute_metrics,\n",
    "            callbacks = [EarlyStoppingCallback(3, 0.0)]\n",
    "            )\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        # print(torch.cuda.memory_summary(device=None, abbreviated=True))\n",
    "        \n",
    "        # try later setup hyperparam here instead of wandb\n",
    "        # # https://huggingface.co/docs/transformers/hpo_train\n",
    "        # if hyperparam_search:\n",
    "        #     best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")\n",
    "        #     print(f\"best run for {self._model_checkpoint} is {best_run.hyperparameters.items()}\")\n",
    "        #     for n, v in best_run.hyperparameters.items():\n",
    "        #         setattr(trainer.args, n, v)\n",
    "\n",
    "        trainer.train()    \n",
    "        print(f\"free space by deleting: {out_dir}\")\n",
    "        shutil.rmtree(out_dir, ignore_errors=True)\n",
    "        \n",
    "        if get_pred:\n",
    "            print(f\"\\nGetting Predictions on Test dataset\")\n",
    "            logits, labels, metrics = trainer.predict(self._test_dataset)\n",
    "            predictions = np.argmax(logits, axis=-1)\n",
    "            return trainer, (labels, predictions)    \n",
    "        else:\n",
    "            return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a7ef1-8ccc-4913-9b09-da592e170fbe",
   "metadata": {},
   "source": [
    "## Evaluater \n",
    "<a name=\"evaluater\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceda8f52-3fc2-4a51-8544-414a82f1d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4-*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "1. Evaluate the model performance per language.\n",
    "2. Evaluate performance per category during each epoch.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluation_display(y, y_pred, labels_map=None, plot=False):\n",
    "    \n",
    "    if labels_map is not None:\n",
    "        labels_idx = [k for k,v in labels_map.items() if k in y.unique()]\n",
    "        labels_name = [v for k,v in labels_map.items() if k in y.unique()]\n",
    "    else:\n",
    "        labels_idx = None\n",
    "        labels_name = None\n",
    "        \n",
    "    f1 = f1_score(y, y_pred, labels=labels_idx, average='macro')\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    class_report = classification_report(y, y_pred, labels=labels_idx, target_names=labels_name)\n",
    "\n",
    "    # y_score = pred probabilityes\n",
    "    # fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    # roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "    # cm = multilabel_confusion_matrix(y, y_pred)\n",
    "    cm = confusion_matrix(y, y_pred, normalize='true')\n",
    "    \n",
    "    if plot:\n",
    "        print(f\"f1: {f1}\\nacc: {acc}\\n{class_report}\")\n",
    "        fig, ax = plt.subplots(figsize=(4,7))\n",
    "        cm_disp = ConfusionMatrixDisplay(cm).plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "        c_bar = fig.add_axes([ax.get_position().x1+0.01, ax.get_position().y0, 0.05, ax.get_position().height])\n",
    "        plt.colorbar(cm_disp.im_,  cax=c_bar)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"f1: {f1}\\nacc: {acc}\\n{class_report}\\n{cm}\")\n",
    "    \n",
    "class PredictionEvaluater(object):\n",
    "\n",
    "    def __init__(self, prediction_set, target_names=None, savepath=None, model_name=None):     \n",
    "\n",
    "        self._labels, self._predictions = prediction_set\n",
    "        \n",
    "        self._target_names = target_names\n",
    "        self.num_labels = len(target_names)\n",
    "        self.label_map = {k:v for k,v in zip(range(self.num_labels), self._target_names)}\n",
    "        print(f\"Number of labels in target_names is {self.num_labels}\")\n",
    "\n",
    "        self.savepath = savepath.joinpath('predictions')\n",
    "        self.savepath.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self._model_name = model_name if model_name is not None else \"model\"\n",
    "        \n",
    "    def compute_metrics(self, eval_pred, eval_metric=\"accuracy\"):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        metric = evaluate.load(eval_metric)\n",
    "        return metric.compute(predictions=preds, references=labels)\n",
    "    \n",
    "    def append_predictions(self, test_df):\n",
    "        # append prediction and encoded labels to original test data\n",
    "        df = test_df.copy()\n",
    "        df[f\"{self._model_name}_prediction\"] = self._predictions\n",
    "        df[\"annotation\"] = self._labels\n",
    "        if self.savepath is not None:\n",
    "            sub_df = df[[f\"{self._model_name}_prediction\",\"annotation\"]]\n",
    "            sub_df.rename_axis('index').to_csv(self.savepath.joinpath(f\"{self._model_name}_predictions.csv\"))\n",
    "        return df\n",
    "        \n",
    "    def evaluation_report(self, test_df, lang_eval=True):\n",
    "        df = self.append_predictions(test_df)\n",
    "        evaluation_display(df[\"annotation\"], df[f\"{self._model_name}_prediction\"], self.label_map)\n",
    "        # if language evaluation is required\n",
    "        if lang_eval:\n",
    "            for lang, lang_df in df.groupby(\"lang\"):\n",
    "                print(f\"\\nEvaluation for language: {lang}\")\n",
    "                evaluation_display(lang_df[\"annotation\"], lang_df[f\"{self._model_name}_prediction\"], self.label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ec837-3e56-48ab-9b80-5428adee3b16",
   "metadata": {},
   "source": [
    "## Wrapper \n",
    "<a name=\"wrapper\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d86e791e-5b17-43d4-ad24-ae6fb76098ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "The script for wrapper function to run multilingual_ILI_classification.py\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import logging as log\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# from reader import EncodedDataset\n",
    "# from preprocessor import DataProcessor\n",
    "# from trainer import hyperparams, ModelTrainer\n",
    "# from evaluater import PredictionEvaluater\n",
    "\n",
    "def getsplitidx(lang_split_idx, key='train_idx'):\n",
    "    idx_list = [v[key] for k,v in lang_split_idx.items()]\n",
    "    # print(len(idx_list))\n",
    "    idx_list = [i for eachlist in idx_list for i in eachlist]\n",
    "    return idx_list\n",
    "\n",
    "def getsplit(lang_split_idx, tweets, savepath, save=False):\n",
    "    \n",
    "    # get train, valid and test split for selected languages\n",
    "    train_idx = getsplitidx(lang_split_idx, key='train_idx')\n",
    "    valid_idx = getsplitidx(lang_split_idx, key='valid_idx')\n",
    "    test_idx = getsplitidx(lang_split_idx, key='test_idx')\n",
    "    print(f\"Distribution of data in train, validation and test splits: {len(train_idx)}, {len(valid_idx)}, {len(test_idx)}\")\n",
    "    \n",
    "    # save test set for evaluation later\n",
    "    train_df = tweets.data.iloc[train_idx]\n",
    "    valid_df = tweets.data.iloc[valid_idx]\n",
    "    test_df = tweets.data.iloc[test_idx]\n",
    "    if save:\n",
    "        test_df.rename_axis('index').to_csv(savepath)\n",
    "\n",
    "    return train_df, valid_df, test_df\n",
    "    \n",
    "def mlm_evaluation(train_df, valid_df, test_df, config, lang, lang_eval, split_path, cache_path, hyperparams=hyperparams):\n",
    "    \n",
    "    # ensure all parameters for trianing exists\n",
    "    CONFIG = {k.upper():v for k,v in config.items()}\n",
    "    if not all(k in CONFIG.keys() for k in hyperparams):\n",
    "        sys.exit(f\"provide all required hyperparams: {hyperparams}. Received only {CONFIG.keys()}\")\n",
    "        \n",
    "    # combine data for preprocessing\n",
    "    alldata = pd.concat([train_df, valid_df, test_df])\n",
    "    original_index = alldata.index\n",
    "    alldata.reset_index(inplace=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # process data into encoded features and labels and then to dataset object\n",
    "        print(f\"Working with {CONFIG['MODEL_CHECKPOINT']}\")\n",
    "        processor = DataProcessor(CONFIG, return_type_ids=True)\n",
    "        feature_encodings, label_encodings = processor.encoded_data(alldata['tweet'], alldata['final_annotation'])\n",
    "        encoded_dataset = EncodedDataset(feature_encodings, label_encodings)\n",
    "    \n",
    "        # for now unable to use hugging face trainer without validation set\n",
    "        # refer to Training arguments and then change if required to train on train+valid data\n",
    "        \n",
    "        # train with data splits and configurations provided\n",
    "        n, n_train, n_valid, n_test = alldata.shape[0], train_df.shape[0], valid_df.shape[0], test_df.shape[0]\n",
    "        train_dataset, valid_dataset, test_dataset = encoded_dataset.splitdata([range(n_train), \n",
    "                                                                                range(n_train, n_train+n_valid), \n",
    "                                                                                range(n_train+n_valid, n)\n",
    "                                                                               ])\n",
    "        print(f\"Distribution of data splits for {lang} language is {train_dataset.shape}, {valid_dataset.shape}, {test_dataset.shape}\")\n",
    "        trainer = ModelTrainer(CONFIG, train_dataset, valid_dataset, test_dataset, processor.tokenizer(), split_path, cache_path, lang)\n",
    "        model, prediction_set = trainer.train_eval(get_pred=True)\n",
    "        print(f\"\\n{trainer.model_name} trained on {trainer._lang_to_train} languages\")\n",
    "            \n",
    "        # evaluate on test set\n",
    "        evaluater = PredictionEvaluater(prediction_set, CONFIG['TARGET_NAMES'], split_path, f\"{trainer.model_name}_{lang}\")\n",
    "        evaluater.evaluation_report(test_df, lang_eval)\n",
    "     \n",
    "    except Exception as error:\n",
    "        print(\"An error occurred:\", error)\n",
    "        log.exception('Failed')\n",
    "        pass \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"\\nTotal execution time to finetune {CONFIG['MODEL_CHECKPOINT']} on {lang} language(s) is {execution_time}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab6589-fbbe-4fd3-a8ab-c60bcb8e6994",
   "metadata": {},
   "source": [
    "## Load Data \n",
    "<a name=\"loaddata\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "329fbe23-ba08-42c7-ae8d-33335b5631c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in data: 4400\n",
      "Distribution of classes in all data final_annotation\n",
      "3. Not Related to ILI or COVID-19 Infection       2492\n",
      "1. Likely ILI infection                           1553\n",
      "4. Ambiguous/Unsure                                238\n",
      "2. Likely COVID-19 Infection (after 2020 only)     117\n",
      "Name: count, dtype: int64\n",
      "Configuration setup read from /gaueko0/users/nmishra/multiling_fludetection/params.tsv\n",
      "Cache in /gaueko0/users/nmishra/multiling_fludetection/.cache\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "The script to run model once the best hyperparameters are identified using classification_wandb.py\n",
    "Update required in final_configs.json \n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from pathlib import Path \n",
    "import argparse\n",
    "import json\n",
    "import logging as log\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "# from reader import CustomDataset\n",
    "# from wrapper import mlm_evaluation\n",
    "\n",
    "# # args\n",
    "# parser = argparse.ArgumentParser(description=\"Twitter Meta Analysis\")\n",
    "# parser.add_argument(\"--data_file\", type=str, help=\"File name including directory where data resides.\")\n",
    "# parser.add_argument(\"--params_file\", type=str, help=\"File where parameters to run the model are provided.\")\n",
    "# parser.add_argument(\"--output_dir\", type=str, help=\"Directory where output are to be stores.\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# DATA_FILE = Path(args.data_file)\n",
    "# PARAMS_FILE = Path(args.params_file)\n",
    "# OUT_PATH = Path(args.output_dir)\n",
    "\n",
    "DATA_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/data/all/alldata.csv\")\n",
    "PARAMS_FILE = Path(\"/gaueko0/users/nmishra/multiling_fludetection/params.tsv\")\n",
    "OUT_PATH = Path(\"/gaueko0/users/nmishra/multiling_fludetection/eval_new\")\n",
    "\n",
    "# read data\n",
    "data_path = DATA_FILE.parent\n",
    "tweets = CustomDataset(DATA_FILE, data_path)\n",
    "print(f\"Number of tweets in data: {tweets.__len__()}\")\n",
    "print(f\"Distribution of classes in all data {tweets.labels.value_counts()}\")\n",
    "\n",
    "# hyperparameters\n",
    "params = pd.read_csv(PARAMS_FILE, sep='\\t')\n",
    "# params['split'].apply(ast.literal_eval)\n",
    "print(f\"Configuration setup read from {PARAMS_FILE}\")   \n",
    "\n",
    "# where MLMs are cached\n",
    "cache_path = OUT_PATH.parent.joinpath('.cache')\n",
    "cache_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Cache in {cache_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d4e1c-876e-466c-a951-03b9e7cc4757",
   "metadata": {},
   "source": [
    "## Do Training \n",
    "<a name=\"dotraining\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d8f73d8-ab95-48e9-994a-0108d12453fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data split index from: /gaueko0/users/nmishra/multiling_fludetection/eval_new/testset0.6_0.2_0.2\n",
      "\n",
      "Train using data from all languages\n",
      "Distribution of data in train, validation and test splits: 2640, 880, 880\n",
      "Working with cardiffnlp/twitter-xlm-roberta-base\n",
      "\n",
      "Final configurations for processing training + validation data\n",
      "{'LANG': 'all', 'SPLIT': '0.6,0.2,0.2', 'N_LABELS': 4, 'MAX_LEN': 128, 'MODEL_CHECKPOINT': 'cardiffnlp/twitter-xlm-roberta-base', 'BATCH_SIZE': 32, 'EPOCHS': 1, 'LEARNING_RATE': 3.571430010972717e-05, 'TARGET_NAMES': ['1. Likely ILI infection', '2. Likely COVID-19 Infection (after 2020 only)', '3. Not Related to ILI or COVID-19 Infection', '4. Ambiguous/Unsure']}\n",
      "Dimensions of encoded features: torch.Size([4400, 128])\n",
      "Encoding contains: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Distribution of data splits for all language is (2640, 4), (880, 4), (880, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 classes in all language\n",
      "Model to be saved in /gaueko0/users/nmishra/multiling_fludetection/eval_new/testset0.6_0.2_0.2/models/twitter-xlm-roberta-base-all-finetuned\n",
      "\n",
      "Final configurations for training the model:\n",
      "namespace(max_len=128, model_checkpoint='cardiffnlp/twitter-xlm-roberta-base', batch_size=32, epochs=1, learning_rate=3.571430010972717e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='83' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58/83 00:17 < 00:07, 3.25 it/s, Epoch 0.69/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m training_params:\n\u001b[1;32m     32\u001b[0m     config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_names\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m target_names\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mmlm_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 67\u001b[0m, in \u001b[0;36mmlm_evaluation\u001b[0;34m(train_df, valid_df, test_df, config, lang, lang_eval, split_path, cache_path, hyperparams)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribution of data splits for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m language is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dataset\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_dataset\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_dataset\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(CONFIG, train_dataset, valid_dataset, test_dataset, processor\u001b[38;5;241m.\u001b[39mtokenizer(), split_path, cache_path, lang)\n\u001b[0;32m---> 67\u001b[0m model, prediction_set \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trained on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39m_lang_to_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m languages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# evaluate on test set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 132\u001b[0m, in \u001b[0;36mModelTrainer.train_eval\u001b[0;34m(self, get_pred, metric_name, hyperparam_search)\u001b[0m\n\u001b[1;32m    121\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# print(torch.cuda.memory_summary(device=None, abbreviated=True))\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# try later setup hyperparam here instead of wandb\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#     for n, v in best_run.hyperparameters.items():\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#         setattr(trainer.args, n, v)\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfree space by deleting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m shutil\u001b[38;5;241m.\u001b[39mrmtree(out_dir, ignore_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/niti_venv/trumoi-transformers-4.20/lib/python3.9/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/niti_venv/trumoi-transformers-4.20/lib/python3.9/site-packages/transformers/trainer.py:1862\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# read data split index\n",
    "SPLITS = params['split'].unique()\n",
    "for split in SPLITS:\n",
    "    dirname = f\"testset{split.replace(',','_')}\"\n",
    "    split_path = OUT_PATH.joinpath(dirname)\n",
    "    print(f\"Reading data split index from: {split_path}\")\n",
    "    with open(split_path.joinpath('split_idx.json'), 'r') as f:\n",
    "        split_idx = json.load(f) \n",
    "   \n",
    "    for lang, lang_params in params.groupby('lang'):      \n",
    "        # get split idx \n",
    "        if lang=='all':\n",
    "            # for all languages in the list\n",
    "            print(f\"\\nTrain using data from all languages\")\n",
    "            languages = [i for i in params['lang'].unique() if i!= 'all']\n",
    "            lang_split_idx = {i:split_idx[i] for i in languages}\n",
    "            lang_eval = True\n",
    "        else:\n",
    "            # for each language in the list\n",
    "            print(f\"\\nTrain using data from {lang} languages\")\n",
    "            lang_split_idx = {}\n",
    "            lang_split_idx[lang] = split_idx[lang]\n",
    "            lang_eval = False\n",
    "\n",
    "        # get train, valid and test split for selected languages\n",
    "        train_df, valid_df, test_df = getsplit(lang_split_idx, tweets, split_path.joinpath(f\"{dirname}_{lang}.csv\"))\n",
    "        \n",
    "        # get training parameters for models and train\n",
    "        target_names = sorted(test_df['final_annotation'].unique().tolist()) # because english lang has only three labels\n",
    "        training_params = lang_params.to_dict(orient='records')\n",
    "        for config in training_params:\n",
    "            config['target_names'] = target_names\n",
    "            mlm_evaluation(train_df, valid_df, test_df, config, lang, lang_eval, split_path, cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b81d987e-bc43-4e26-8b0d-92ca4b903be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # read data split index\n",
    "# splits = params['split'].unique()\n",
    "# for split in splits:\n",
    "    \n",
    "#     cache_path = OUT_PATH.parent.joinpath('.cache')\n",
    "#     cache_path.mkdir(parents=True, exist_ok=True)\n",
    "#     print(f\"Cache in {cache_path}\")\n",
    "\n",
    "#     dirname = f\"testset{'_'.join([str(i) for i in split])}\"\n",
    "#     split_path = OUT_PATH.joinpath(dirname)\n",
    "#     print(f\"Reading data split index from: {split_path}\")\n",
    "#     with open(split_path.joinpath('split_idx.json'), 'r') as f:\n",
    "#         split_idx = json.load(f) \n",
    "        \n",
    "#     # determine languages for which to get split index\n",
    "#     if params['LANG']=='all':\n",
    "#         languages = [i for i in split_idx]\n",
    "#     else:\n",
    "#         languages = [i for i in split_idx if i in params['LANG'].split(',')]\n",
    "    \n",
    "#     # # train on all languages and then on each language\n",
    "#     # lang_split_idx = {i:split_idx[i] for i in languages}\n",
    "#     # print(f\"Training data used for {params['LANG']} languages\")\n",
    "#     # mlm_evaluation(lang_split_idx, tweets, params, split_path, dirname, params['LANG'], cache_path)\n",
    "    \n",
    "#     for lang_to_train in languages:\n",
    "#         if lang_to_train=='es': #or lang_to_train=='it':\n",
    "#             print(f\"\\nTraining data used for {lang_to_train} language\")\n",
    "#             lang_split_idx = {}\n",
    "#             lang_split_idx[lang_to_train] = split_idx[lang_to_train]\n",
    "#             mlm_evaluation(lang_split_idx, tweets, params, split_path, dirname, lang_to_train, cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaeed52b-031a-4f86-aee8-2096dade7030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # read data split index\n",
    "# for split in params['SPLITS'][:1]:\n",
    "    \n",
    "#     cache_path = OUT_PATH.parent.joinpath('.cache')\n",
    "#     cache_path.mkdir(parents=True, exist_ok=True)\n",
    "#     print(f\"Cache in {cache_path}\")\n",
    "\n",
    "#     dirname = f\"testset{'_'.join([str(i) for i in split])}\"\n",
    "#     split_path = OUT_PATH.joinpath(dirname)\n",
    "#     print(f\"Reading data split index from: {split_path}\")\n",
    "#     with open(split_path.joinpath('split_idx.json'), 'r') as f:\n",
    "#         split_idx = json.load(f) \n",
    "        \n",
    "#     # determine languages for which to get split index\n",
    "#     if params['LANG']=='all':\n",
    "#         languages = [i for i in split_idx]\n",
    "#     else:\n",
    "#         languages = [i for i in split_idx if i in params['LANG'].split(',')]\n",
    "    \n",
    "#     # train on all languages and then on each language\n",
    "#     lang_split_idx = {i:split_idx[i] for i in languages}\n",
    "#     print(f\"Training data used for {params['LANG']} languages\")\n",
    "#     # mlm_evaluation(lang_split_idx, tweets, params, split_path, dirname, params['LANG'], cache_path)\n",
    "    \n",
    "#     for lang_to_train in languages:\n",
    "#         print(f\"\\nTraining data used for {lang_to_train} language\")\n",
    "#         lang_split_idx = {}\n",
    "#         lang_split_idx[lang_to_train] = split_idx[lang_to_train]\n",
    "#         mlm_evaluation(lang_split_idx, tweets, params, split_path, dirname, lang_to_train, cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed7447-b0ec-439e-ae0d-89a1ef561bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel ILI Test",
   "language": "python",
   "name": "ipykernel-ili-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
